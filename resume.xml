<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.6 -->
<document ids="id1" names="修士論文「位置提示技術を用いた状況での相互行為の分析」要約" source="resume.rst" title="修士論文「位置提示技術を用いた状況での相互行為の分析」要約"><title>修士論文「位置提示技術を用いた状況での相互行為の分析」要約</title><section ids="id2" names="背景と目的"><title>背景と目的</title><paragraph>本論文では、実世界の環境の提示の中でも、空間に関するものが、離れた環境でどのように用いられるかを、相互行為分析の手法を用いて分析した。この背景には、コンシューマコンピューティングの技術の進展により、新たな遠隔地での共同作業の可能性が開かれたことがある。現在、SOHOやノマドワーキングなどの、特定の組織や場所に縛られない働き方が提唱されている。一方で、このような働き方ができるような業種は、いわゆるホワイトカラーや、ソフトウェア開発など一部に限られる。これは、現在普及しているPCのインタフェースがオフィスワークを目的としたものであることに大きく起因すると考えられる。画像処理、拡張現実感技術や、「タッチ」を中心としたタンジブルなインタフェースが安価に手に入りつつあるが、これを実用的に応用する試みはまだ少ない。本論文では、この可能性を模索することを目的とする。</paragraph></section><section ids="id3" names="技術的概観"><title>技術的概観</title><paragraph>(何かまとめ方がdourishの'Where the action is'に似ているが気にしない)
この方針を、タンジブル・インタフェースおよび遠隔共同作業の観点で整理する。</paragraph><section ids="id4" names="タンジブルなインタフェース"><title>タンジブルなインタフェース</title><paragraph>まず、本論で扱う技術は記号操作の枠に止まらず、実際に手で触ることができる空間に関わり、またそのように操作できるという特徴がある。前者に関しては、空間的広がりや、方向などを含んだものをコンピュータで扱えることを可能にするセンシング技術の進歩が関わっている。後者に関しては、ユーザーの物理的な操作を画像処理や加速度の認識によってインタフェースにできる技術が関わっている。基本的に、いわゆる拡張現実感技術も、単なる3Dの提示だけではなく、実際に手で操作できるという特性を持つ。</paragraph><paragraph>もう一つの特徴が、単なる情報処理ではなく、共同で作業を行えるというものである。(要調べ)</paragraph><paragraph>この2つにより、その場にいなくても、実空間での作業を、できるだけその場にいるのと近い形で共同して行うことができることが、近年の技術の可能性の大きな特徴であることがわかる。一方で、この様な技術や作業には、今までと異なる問題がある。私たちはどのように実際の空間を理解し、お互いにその理解を示しているのか？また、実際に見えて聞こえて触れる空間では、文字によるものとは異なるやり方でそれが行われるはずである。これは遠隔地ではどうなってしまうのか？</paragraph></section></section><section ids="id5" names="分析の方法論と方針"><title>分析の方法論と方針</title><paragraph>以上で取り上げたような技術は、高度に環境に依存し、即時的な特徴を持つ。このため、主要な問題もHCIでメジャーな分析手法である行動科学に基づく手法では、不十分であるかずれていることが考えられる。そこで、本論文ではエスノメソドロジーに基づく相互行為分析の適用を試行する。これは、ビデオによって、その場に居合わせた人々の行為がどのように組織化されているかを記述する手法で、CSCWなどの分野でも比較的応用が多い。以下ではエスノメソドロジーと主要な手法である会話分析について、少なくとも著者の理解を示し、それを元に相互行為分析の特徴と可能性について説明する。</paragraph><section ids="id6" names="概要"><title>概要</title><paragraph>エスノメソドロジーは、単に日常生活を研究するのではなく、それが既に秩序だっているような手続きを研究する分野である。これを実際に記述する手法が会話分析や相互行為分析で、これらは相互行為のシークエンス的な組織化を詳細に明らかにする。これは、その場面である作業を達成するために、どのようにその場その場で成立する秩序を成員が理解し、次の相互行為につなげているかということがわかる。</paragraph></section><section ids="id7" names="エスノメソドロジー"><title>エスノメソドロジー</title><paragraph>(この辺から再構築する)
エスノメソドロジーは、創始者のHarold Garfinkelによって以下のように特徴づけられている。「私が「エスノメソドロジー」という言葉を使う際は、日常生活の組織立った巧妙な実践の、偶発的で継続的な達成としての、文脈指標的表現やその他の実践的行為の規範的特徴の研究を指す」(<citation_reference ids="id8" refid="garfinkel1967">Garfinkel1967</citation_reference>, p.11)。つまり、我々が何かの枠組みをもって行為を説明する以前に、人々の実践的行為はすでに秩序立っている。この秩序を解明することが、エスノメソドロジーの最も基本となる考え方である。とはいえ、エスノメソドロジーは、単に人々の日常を明らかにする、ということではない。(この辺そのうち説明する)</paragraph><paragraph>この議論では、具体的にどう明らかにするのか、というところまでは踏み込んでいない。エスノメソドロジーを具体的にどうやっていくのかということに関しては、当時エスノメソドロジーが大きな影響を与え、またその代表的な研究手法となった会話分析について触れる[#]_。会話分析は、主にSacks, Schegloff, Jeffersonらによって開始された、会話の組織化に関する広範な研究である。会話分析の対象は近年Schegloff2007によって以下のように特徴づけられている。</paragraph><bullet_list bullet="*"><list_item><paragraph>順番交代 (turn-taking) 問題:会話において誰が次に話すのか?またそれはいつ行われるのか?</paragraph></list_item><list_item><paragraph>行為形成 (action-formation) 問題:どのように、言語、身体や、相互行為の環境、相互行為内の位置などのリソースが、設計された通りの構造に、また受け手に、その規模もわからないのに特定の行為 (例えば、依頼、招待、許可、不平、同意、知らせ、警告、拒絶など) として認識されるように形成されるのか?</paragraph></list_item><list_item><paragraph>シークエンス組織 (sequence-organazational) 問題:どのように、次の順番が前の順番と「筋の通った」ものとして形成されるのか?また、そもそも「筋が通った」の本質とは何か?</paragraph></list_item><list_item><paragraph>トラブル (trouble) 問題:どのように話し、聞いたり、会話や相互行為を理解する際のトラブルが、それが起こった際に止まらず、間主観性が維持、修復され、順番やシークエンス、活動が可能な完了へと進むように扱われるのか?</paragraph></list_item><list_item><paragraph>言葉の選択 (word-selection) 問題:どのように順番の単位となる構成要素が選択されるのか?また、どのようにその選択が、受け手が理解を達成できるように知らせ、形成されるのか?</paragraph></list_item><list_item><paragraph>全体構造の組織化 (overall structural organization) 問題:相互行為の出来事の全体的な組織は、どのように組み立てられるのか?その構造とは何か?また、どのように全体構造の配置が、その構造と、シークエンスや順番としての会話を知らせるのか?</paragraph></list_item></bullet_list><paragraph>会話分析においては、会話の録音と、それを文字に起こして分析を容易にするトランスクリプトが分析の基礎になる。先駆的な研究によって、会話の組織化には発話の間や複数の発話のオーバーラップなどが有意であるということが明らかになっている。これらを含めて書き起こせるようにしたのが、Jefferson Systemであり、後の相互行為分析に使われるトランスクリプトでもその拡張が使われている。特有の記号などについては実際の分析で必要なものをその都度説明する。</paragraph></section><section ids="id9" names="相互行為分析"><title>相互行為分析</title><paragraph>(この辺から再構築する)
「相互行為分析」は、主にGoodwin, Heathらによって始められた、会話も含めた身体的相互行為をビデオによって分析する方法である。対面した相互行為では、会話の書き出しだけでは発話のポーズなどを説明できない場合がある。もしくは、会話がなくても何らかの相互行為を組織させる、ということはよくあることである。相互行為分析は、前述の会話分析の拡張ではあるが、環境、指示などのあり方にさらに迫ることができる。</paragraph></section><section ids="id10" names="相互行為分析が明らかにした知見"><title>相互行為分析が明らかにした知見</title></section><section ids="cscw" names="相互行為分析とcscw"><title>相互行為分析とCSCW</title></section><section ids="id11" names="相互行為分析と拡張現実感技術"><title>相互行為分析と拡張現実感技術</title></section></section><section ids="id12" names="システムデザインへの適用の問題"><title>システムデザインへの適用の問題</title><paragraph>(「相互行為分析とCSCW」を書いてから)
相互行為分析などの、エスノメソドロジーに影響を受けた手法(Ethnomethodology-informed Ethnographyや、会話分析なども含む)をどう実際のシステム設計に取り入れるかに関しては、その当初から議論が存在する。</paragraph><paragraph>90年代の論文(Suchman, Button, Hughes etc.)
00年代の解説書(Crabtree, Randall)</paragraph><paragraph>10年の入門書(Button, Heath)
Button「Studies of work and workplace in HCI」
1.motivation
■Grudinの「HCIのfifth stageはユーザーとの対話だ」はwork settingへの注目を意味するが、それはCSCW、特に社会学と共同した分野である。社会学の中でも、経験的なアプローチが理論より好まれる。
■Suchmanは、従来のHCIにおける認知科学的アプローチ、つまりユーザーを単独で見ることに対抗し、「使用」の社会的文化的状況という視点を導入した。一方、CSCW分野でも、人々の共同作業を促進するには、認知科学的モデルは適切でないことがわかった。Suchmanはそれに対してEMCAによる経験的研究という指針を示した。このほか、スカンジナビアのParticipatory Design運動は、技術開発における、ユーザーの作業状況での使用の重要性を指摘しつづけてきた。
2.Overview: A Paradigmatic Case
■HCIに対するワークの研究の適用は、システムへの批判につながる場合がある。Suchman-Winograd論争の事例。Bowersらの研究では、印刷作業が今までどうだったか、システムが導入されたらどう変わったかを分析した。システムが導入されたら、円滑な共同作業が妨げられてしまった。この原因は、設計者がワークフローを強制してしまったためだった。様々な過程は、状況に合わせられなければならない。そのためにうまくいかせるプロセスがあったはずだが、たまたま起こらなかったためにシステムに反映されなかったのだ。
■ワークの研究は、組織化をうまくいかせるやり方を明らかにする。それは、デザイン方針への批判だけでなく、それをうまくいかせることにもつながる。
4.Detailed description
1.批判:Suchman-Winograd論争
2.評価:Disembodied Conduct→読むか
3.要求定義
4.基礎的関係:Technomethodology</paragraph><paragraph>(どうにかする)
なんかどうも界隈で意見が割れている話題として、新しいインタラクティブなものを作る際に、アイデアを重視するか、分析や観察を重視するかというものがある。パソコンでのGUIの発展とか、バーチャルリアリティとかの分野では、伝統的にまじめな工学から少し浮いた人間がいて、そいつがとんでもない発想をして時代を進化させるみたいな風潮がある。それに対し、まじめに数式とか計算とかをして分析をして、改善していくみたいな人たちや、近年の社会的な製品に対応するために社会学からやってきた連中が、こいつらが作っているものは、本当に世の中を良くしているのかわからんということを言い出したのが最近の話。</paragraph><paragraph>結果がどうであるかというと、どっちもどっちである。イマイチなアイデアでも、少し分析と改善を回しただけで凄まじいものになる場合があるし、逆に最初の製品のイメージがないと、分析のプロセスは回らない。典型的なのがAppleとMSで、Appleはアイデア重視にしたとたん爆発し、MSは研究所で分析の専門家をふんだんに入れた結果、地味だが良いものを出し始めてきた。これに関しては甲乙つけがたい。</paragraph><paragraph>で、いろいろなところでいろいろな態度が取られているわけだ。</paragraph><bullet_list bullet="*"><list_item><paragraph>設計と分析を完全に分ける。分析からインスピレーションを得る</paragraph></list_item><list_item><paragraph>自分の目で見たもの、体験したものを克明に記録し、それを設計に取り入れたりブレインストーミングしたりする</paragraph></list_item><list_item><paragraph>分析なんてどうでもいいからアイデアを作ってとにかく出す</paragraph></list_item><list_item><paragraph>最初から作るものは決まっている。あとは分析で洗練させる</paragraph></list_item></bullet_list><paragraph>まあこんな感じが典型かと。この内部でもいろいろあるので、一人一言あるといってもいい。一応デザイン思考とか人間中心設計とかある程度の方針はあるが、ほぼ必ずと言っていいほどアレンジがある。</paragraph><paragraph>一応近年の風潮としては、某国際的に権威のある会議では、アイデアを出すだけのが中心だったのが、分析をちゃんとやるのが通りやすくなっていると聞いた。で、「安易に参与観察とか取り入れるのはどうよ」みたいなセッションが中にある。</paragraph><paragraph>問題は、別の立場の人々と組む場合である。私が今まさにそれを考えているところである。私は基本的に社会学の人間である。しかし、過去のしがらみからバーチャルリアリティに関する制作物、コンセプトを出しているという感じである。だから、一応私個人で制作から分析まで見通せることにはなる(実際はとても無理)が、それでは単純に体が持たない。</paragraph><paragraph>今考えている態度としては、どうせみんな設計に対する立場が違うのだから、共同作業ではなく分業という側面でとらえるとうまく行くんじゃないかと思う。例えば、全く新しい技術コンセプトなどを出す場合、技術自体が定まっていないのだから、アイデアが主になる。一方、ある新技術が決まっていて、それを特定の場面に適用していくとなると、Workplaceの分析が不可欠である。しかし、この2つは矛盾しないし、ある程度の情報交換があれば平行して行うこともできるし、お互いにとってリソースとなる。</paragraph><paragraph>要は、インタラクティブなものに関わってる人は、まじめなやつにしても変なやつにしてもみんなアクが強いから、「何を作るか、分析するか」についてコンセンサスを得る必要がないし、互いになんか似たようなことをやって影響を与え合うのが良い。以前のように「いろんなアイデアを持った人がいて、アイデア同士が影響し合う」という時代ではなく、「いろんな態度を持った人がいて、分業を意識しないと話が通じない」という妥当な結論。</paragraph><section ids="hci" names="hciとの関連における初期の議論"><title>HCIとの関連における初期の議論</title></section><section ids="id13" names="成果の蓄積と実践的な利用"><title>成果の蓄積と実践的な利用</title></section><section ids="id14" names="これらの議論に影響を及ぼしうるいくつかの新しい設計論"><title>これらの議論に影響を及ぼしうるいくつかの新しい設計論</title></section></section><section ids="fieldwork-geogeo-stamp-rally" names="fieldwork:\ geogeo\ stamp\ rally"><title>Fieldwork: Geogeo Stamp Rally</title><paragraph>現在，iPhoneやスマートフォンなどの高度な携帯電話端末が，一般ユーザーに普及している段階にある．これらは，通話やメールなどの枠を遥かに超え，「セカイカメラ」などの位置に対応した情報をカメラ映像に重ねる技術など，従来からMixed Realityと分類されてきた技術を，エンドユーザーにまでもたらしつつある．現在は未だ普及の段階にまで達していないが，実世界とオンラインを結びつける試みに，携帯電話は今後も重要な役割を果たす可能性がある．</paragraph><paragraph>一方で，実世界の環境で，携帯端末がどう使われるかに関しては，十分な検討がされていないと見られる．携帯電話には，一人で画面に向き合うだけではなく，例えば電車内で若者が携帯電話に表示されたメール，画像などを見せあっているように，複数人で，場面に応じて共同的に利用するものとしての側面がある．本論文では，実際に携帯端末がどのように複数人によって，実世界の場面の組織化に利用されるかに関して，詳細な分析を行う．</paragraph><section ids="id15" names="フィールドについて"><title>フィールドについて</title><paragraph>屋外での情報機器の使用を観察する際は，公共のイベントなどの利用が有効である．実際の研究としては，Can You See Me NowというMixed Reality Gameの分析が挙げられる．2009年現在，国内ではその一種と言えるiPhoneを利用した位置情報ゲームが複数行われ始めている．</paragraph><paragraph>本研究では，「ジオジオスタンプラリー」という，レーダーのような形式で提示されたポイントの情報やヒントを頼りに，宝探しを行うゲームの調査を行った．これは2009年7月20日に行われた，全体で50人程度が参加したイベントである．</paragraph><paragraph>参加者はGPSの専門スタッフ1人を含む5人程度の8つのチームに分かれ，各チームにiPhoneが1台配布された．iPhoneにはDGRadar（図）がインストールされており，それを用いてゲームを行う．DGRadarはGPSで現在位置を取得し，レーダーのように現在位置を中心として，周辺（拡大縮小可）の登録されたポイントへの方角・距離と画像などの付加情報が表示されるアプリケーションである．</paragraph><paragraph>実際に行われたゲームは，（１）立教大学キャンパス内での人形探し（２）都電沿線でのスタンプラリーの2つであったが，本論文に関連する前者についてのみ記す．人形は1cm程度の高さのアヒルであり，マグネットによって金属部分に接着可能である．この人形がキャンパス内の5カ所に配置され，それぞれのポイントの位置情報のみがDGRadarに登録された．</paragraph><paragraph>各チームはこのアヒルを30分程度で可能な限り見つけるというルールであるが，特に勝敗などを決めるものではなく，純粋に楽しむ目的のものであった．ゲームの終わりに全員集合し，各チームの結果や動いた軌跡などを主催者が発表した．</paragraph><paragraph>本イベントには，田島が技術サポートの集団の一人として参加しており，その中で企画者に調査の提案をした．参加者には最初に集合した際に調査内容に関して説明を行い，全員に口頭で撮影の許可を得た．その後，1チームに対して全体で30分程度，小型のデジタルムービーカメラを用いて追跡して撮影を行った．このチームでは，持参のものと含めて2台のiPhoneを用いていた．</paragraph></section><section ids="id16" names="分析"><title>分析</title><paragraph>本研究では，携帯端末の使用を，人々の共同作業の相互行為的な達成の観点で分析した．すなわち，単に一人で画面に向き合い，画面上の情報とインタラクションを図るというだけでなく，周囲の環境/人間と協調しながら，実世界に関係する作業を達成していくという観点である．</paragraph><paragraph>共同作業の達成を分析するにあたり，社会学のエスノメソドロジー的な相互行為分析の手法を用いた．これは，ビデオデータなどを用いて，その場に居合わせた人間の会話，指さしなどの身体的な相互行為が，継起的な秩序の中でどのように組織化されるかを分析する手法である．本研究では，特にiPhoneやその使用が，環境の中でどのように見られ，相互行為の中に組み込まれていくかに焦点を当てる．</paragraph><section ids="id17" names="指さしによる環境の指示"><title>指さしによる環境の指示</title><paragraph>Goodwinは，環境の特定の対象を指す種類の指さしをSymbiotic Gestureとし，会話と全く異なる記号であるが，会話と協調して使われるものとしている．「ジオジオスタンプラリー」で見られた指さしは20件あったが，そのうちの10件がDGRadarを参照した「方角」の指示であった．典型的なものを断片1（図）に示す．以下では，Aの持つiPhoneをiA，Bの持つものをiBとする．</paragraph><paragraph>(Datas)</paragraph><paragraph>Aは自身のiPhoneを見ながら，次のポイントを発見して報告する．Bはそれを受け，Aの方向を向いて歩き始める．その途中で，AはiPhoneを継続して見ながら，ポイントについてもう一度報告し，一度iPhoneから目を離してポイントの方向を指差し，またiPhoneに視線を戻す．Bはそれを受け，指さしの方向を見てから二人とも歩き始める．</paragraph><paragraph>ここで注目する点が，断片1の2,3行目でAが自身のiPhoneを見ているということを，Bが見ているということである（図）．これにより，Bはその後の指さしがDGRadarの提示するポイントを指していることを理解できる．「向こうに」に伴った指さしは，特定の物体や，道路に沿って指したものではない．iPhoneの，方角を提示するDGRadarを見ているということを見た上で，方角を提示していると，意味のある形で理解できるのである．</paragraph><paragraph>「方角」と，進むべき「方向」は相互行為の中で明確に区別されていた．DGRadarを見た後の指さしと共に「曲がってってもいいんじゃない」という発話を行い，その後チームで建物を迂回する例が見られた．指さしは表示の方角を指しているが，その先には建物があった．このため，「あっち」「東」などの方角ではなく，「曲がってって」という発話が行われた．方角を，進むべき方向に再構成して発話を行ったのである．</paragraph><paragraph>iPhoneを見ているということにより，見ている人の体の向きが，DGRadarの方角を指していると見られた場合があった．ある場面では，Aは最初道路に沿って歩いていたが，iPhoneを覗き込んで横を向いた．それを見た他のメンバーが，向いている方向に歩き始めてしまった．それを受け，Aは「あ，違う，真向こう，真向こう，真向こう，向こう」と訂正を行い，本当にDGRadarが提示している方角を指さす．この場面ではAの見ているiPhoneと，メンバーが利用する資源であるAの体の向きという，2つの異なるエコロジーが問題を起こしている．</paragraph><paragraph>以上のように，ジオジオスタンプラリーではiPhoneを見ていることと，指さしや身体的配置は，関連づけられて理解されていた．</paragraph></section><section ids="iphone" names="2台のiphoneによる問題解決の試み"><title>2台のiPhoneによる問題解決の試み</title><paragraph>ほとんどのチームで，GPSの精度の問題が発生していた．GPSの誤差は明確には表示されていなかったが，チームの相互行為の中で，複数のiPhoneを用いて明らかにした部分があった．断片2（図）はもともと進んでいた方向の異常に気づき，集合する直前のデータ，断片3（図）は集合してから問題解決を始めたデータである．</paragraph><paragraph>(Datas)</paragraph><paragraph>当初2人が別のiPhoneを持って歩いており，Aが指さしで先導していた．しかし，BがAの指差しの方向を見て，iBと照らし合わせ，Aに見える形でiBを指差す．Aは止まりiAを見て，BはiBを見ながらAに向かって歩き始める．それを受けてチーム全員が集合する．</paragraph><paragraph>集合後，1行目の発話で，Bの胴の向きがAのiPhoneへ向かい始める．Bの「北」の発話の段階では，Bは自身のiBを見ているが，iAを見て「きた？」と言いiAを指差す（図6）．その後ジェスチャーで2台の向きの違いを指摘し，iAの指す方角を聞く．それを受けたAの「イースト」の発話と指さしの後，iPhoneをBに手渡し，並べて見る．そこで初めて，専門家であるCが衛星状態について述べる（13行目）．</paragraph><paragraph>注目する点は2つある．まず，どのようにBがAのiPhoneを参照する状況ができたかである．集合前に既にBはiBの異常を示していたが，01行目と胴の動きでiAを見る準備がされている．その後，「北」でiBの表示の具体的な内容を示す．その後の「きた？」でiAを指差したことで，iAとiBの違いが示される．</paragraph><paragraph>次が，2台のiPhoneの比較である．iAとiBの表示の違いは理解されていたが，具体的にどう違うのかは，恐らく2台のiPhoneの向きの違いから，直観的にはわかりにくかった．03行目のなぞる動きや，06行目の「どっちなんですか」10行目の「てーと」という疑問がそれを示している．その直後，AはiAをiBと平行になるようにBに渡す（図7）．2つのiPhoneの示す方角は，既に「北」「イースト」で示されている．しかし，精度を問題にする場合，2台を比較可能，つまり平行にすることが必要であった．Cによる専門的な指摘は，2人の比較を見た直後である．</paragraph></section></section><section ids="id18" names="まとめ"><title>まとめ</title><paragraph>本調査では，GPSを用いた宝探しゲームの中でiPhoneが環境の中でどのように理解され，複数人の相互行為の中に組織化されていくかを分析した．以下に分析の知見をより一般的な形でまとめる．</paragraph><bullet_list bullet="*"><list_item><paragraph>携帯端末を見たり操作していることは，他の参加者が見ることができ，使用者の身体的相互行為は携帯端末に関連したものとして理解された．</paragraph></list_item><list_item><paragraph>身体的配置により，誰かが使っている携帯端末は他の参加者にも利用可能になった．</paragraph></list_item><list_item><paragraph>複数の端末などがある場合，それらの配置が問題になり，調整される場合がある．また，それも見ることができる．</paragraph></list_item></bullet_list><paragraph>本分析の知見は，ゲームという特殊な設定の元でのものであるが，携帯端末を見ながら何かを行うということは，位置情報に限らず表示された文書，画像などに関連したものであることが示唆される．例えば「セカイカメラ」の場合，表示されたエアタグを実際に見なくても，ある程度近くにいれば，体の向きからどの方向のエアタグを見ているのか瞬時に理解できる．</paragraph><paragraph>また，例えばiPhoneの場合電子コンパスや加速度センサで，表示を回転させることが可能であるが，これらは持っている人の向きのみを反映でき，他の人間の身体の志向の反映は難しい．場合によっては渡すなどのインタフェース外の相互行為を考慮した設計も必要だろう．このように，本知見を通じて既存のシステムを再検討することも有効である可能性がある．</paragraph><paragraph>(オチる)</paragraph><section dupnames="これによって何がわかったのか？" ids="id19"><title>これによって何がわかったのか？</title><paragraph>このフィールドで行われたことは、ゲームであり、位置や方向の特定という問題の解決であり、iPhoneの使用である。これらは単純に平行しているわけではなく、例えばゲームで点を取るために位置や方向を特定し、iPhoneを使用することでゲームを進めるなど相互に関係している。本分析でピックアップした断片では、iPhoneの使用を取り巻く指差しなどの身体的相互行為に主に注目した。しかし、これはiPhoneでの情報の提示が間違っているという批判にはならない。また、ゲーム全体に関わるような意思決定も主題としていない。このため、主に位置や方向の特定という問題がどのように解決されるか、ということが本分析の主要な知見だろう。これは、より外部環境のデータをセンシングして、提示するようなシステムでは身振りのあり方を考慮でき、またそれが実際に使用される場面で異なっていくということを示している。この点で、新たなシステムへの要求事項を扱っていると言える。</paragraph><paragraph>一方で、この分析では本当にゲームという場面全体を記述できなかったのだろうか。宝探しという主題を元に、我々は様々な場面を想像するだろう。しかし、今回は場面で起こりうる様々な局面を厳密に洗い出し、行為のモデルを作成し、ゲームをデザインしたというわけではない。つまり、ある意味で実際に始まってみないと、ゲームで起こることは予測できないことになる。これはプレイヤーにとっても同様である。この分析で何か場面について分かったものがあったとすれば、それはまだ知られていない事柄である。</paragraph><paragraph>そこでまず指摘できるのが、アヒル探しがチームの共同作業として行われたことである。これは注目に値する。例えば完全に障害物がない状況で、GPSの方角指示を元に移動を行ったとしたら、各人は同じ方向に進むため、コミュニケーションは必要ないと思われる。人が集まったら共同作業がされるとは限らない。</paragraph><paragraph>そこでゲームを一種の問題解決としてとらえた場合、問題とは何かということを問うことができる。前半のキャンバス内でのアヒル探しと、後半の都電沿線での宝探しではどう問題が異なるだろうか。例えば、ゲームのルールとDGRadarを元にすれば、「方向」の問題は見えてこない。また、GPSの不具合がゲームの障害となることは容易に想像できるが、実際にゲームをどう妨げたのか、また本当に妨げたかどうかには疑問が残る。GPSの問題をお互いに共有して、方向を見定めながら移動するということは、ゲームのルールを破壊するようなことではない。むしろ、ゲーム全体の問題解決の中で、間違えながら試行錯誤していく過程の中にうまく取り込まれている。このように、「iPhoneの位置表示アプリを使った」「宝探しゲーム」の見えない特徴が本分析によって明らかになっている。</paragraph><paragraph>この際、本分析はゲームの実際の達成の際の(ゲームのデザインが問題を解決するものではなく、問題をうまく作り出すことにあるという差異はあれど)問題を浮き彫りにしている。これは、ゲームの評価をしているといえ、この結果は例えば方角ではなく方向を提示してみる、GPSにわざと誤差を作っておくなどの、新たなゲームデザインにつなげることができる。</paragraph></section></section></section><section ids="experiment-augmented-panorama-viewer" names="experiment:\ augmented\ panorama\ viewer"><title>Experiment: Augmented Panorama Viewer</title><paragraph>本章では、2010年7月に行った実験「パノラマを用いた共同作業」を取り扱う。</paragraph><section ids="id20" names="コンセプト"><title>コンセプト</title><paragraph>遠隔で共同作業を行う手段には、様々なものがある。例えば音声や文字(チャット)、映像などは従来から利用されている。本実験で用いられたものは、その中でも「ものを配置する」ということにフォーカスを当て、そのために「パノラマ」すなわち360度全ての方向を写した映像を利用することを考えた。</paragraph><paragraph>この表示の形式は、葛岡、山崎らによる一連のGestureManの研究に影響を受けた。GestureManでは、Body Metaphorという設計思想により、首に配置されたカメラを動かして様々な方向を見ることができる。このため、首の動きを見ることで指示者がどこを見ているか作業者が見ることができ、円滑な指示が可能になる。一方で、現状でロボットは比較的大きなものになるため、作業場所によっては導入できるとは限らない。このため、別のインタラクションを、似たような設計論で実現できないかということを検討した。結果として首を回すかわりにパノラマの提示を、またパノラマを見ている位置を視覚的に提示する方針を採用した。</paragraph><paragraph>360度の映像は、以下のような利点から、ものの配置に有用であるように見える。</paragraph><bullet_list bullet="*"><list_item><paragraph>配置を行う場所の全景を見ることができる</paragraph></list_item><list_item><paragraph>作業者と物体、配置場所の位置関係を把握することができる</paragraph></list_item><list_item><paragraph>作業者に指示を行う際に、場所のどこを指すかをわかりやすく説明できる可能性がある</paragraph></list_item></bullet_list><paragraph>一方で、以下のような問題も起こる。</paragraph><bullet_list bullet="*"><list_item><paragraph>パノラマをどう表示するか？ - パノラマは元々全ての方向を写したものであるため、ただ広げただけでは、位置関係がわかりにくい</paragraph></list_item><list_item><paragraph>パノラマの特定の部分を見ながら指示をしていることを、どう作業者に伝えるか？</paragraph></list_item></bullet_list><paragraph>このような問題を解決するために、パノラマを円筒形に表示する形式を採用した。TWISTARに代表される、没入型で360度の視野を確保するシステムでは、人が円筒の中に入り、中から何らかの形で表示された360度の映像を見るという形式をとっている。しかし、この形式では装置が大規模になってしまい、場所をとってしまうという問題がある。このため、本実験で用いた表示形式は、円筒に360度の映像が表示されているのを、外から見る形式を採用した。</paragraph><paragraph>これを実現するために、拡張現実感技術を用いた。ここで用いた拡張現実感技術は、ARToolKitというマーカーを使ったシステムで、民生用として一般的に用いられているものである。ARToolKitでは、以下のようなフローで現実空間に3Dの物体を表示する。</paragraph><bullet_list bullet="*"><list_item><paragraph>カメラなどで映像のフレームを読み込む</paragraph></list_item><list_item><paragraph>画像認識により、マーカーの位置を特定する</paragraph></list_item><list_item><paragraph>マーカーの位置を原点として、映像に写っている空間の3次元座標を特定する</paragraph></list_item><list_item><paragraph>3次元空間に3Dの物体を描画する</paragraph></list_item></bullet_list><paragraph>この3Dの物体を円筒にし、随時パノラマ映像をテクスチャマッピングすることで、先のような表示形式を実現した。これにより、マーカーが表示された位置に、円筒形のパノラマが表示される。マーカーを見る方向を変えたり、回したりすると、パノラマの別の方向を見ることができる。この方式のもう一つの利点は、パノラマのどこを見ているかを画像処理によって特定できるということである。画面の下方向が3Dのどの方向に当たるかを見ることで、ユーザーがどこを見ているかを推定し、作業者に提示することができる。しかし、この特徴は実際には時間の関係から実装しなかった。</paragraph></section><section ids="id21" names="システムの概要"><title>システムの概要</title><paragraph>実際に実装したシステムは、指示者側、作業者側の2つに大きく分かれ、この2つをネットワークで接続することで実現している。</paragraph><paragraph>まず、作業者側では、PCにWebカメラが接続され、パノラマ映像のキャプチャと送信を行う。パノラマ映像は、通常は全方位カメラ(Omni-Directional Camera)という特殊なカメラを用いるが、今回は予算の問題から(本研究は一切大学からの予算を用いていない)、市販のWebカメラと半球ミラーから自作した。WebカメラはLogicool QCAM-200Vを用いた。半球ミラーは、新宿東急ハンズで販売されているいくつかの口径のものを試し、直径7cmのものを採用した。まず半球ミラーを机などの上に設置し、Webカメラを真上から見下ろすように、ちょうど良い高さに設定すればパノラマ映像を取得できる。</paragraph><paragraph>これを、PCでOpenCVという画像処理ライブラリによってキャプチャし、送信するプログラムを作成した。転送の形式はリアルタイム処理の実現のため、無圧縮でそのままフレームを送信している。</paragraph><paragraph>指示者側ではPCに一眼デジタルカメラ(ビデオキャプチャにより接続)が接続され、受け取ったパノラマ映像をARToolKitによってマッピングする処理を行う。一眼デジタルカメラは近くの机に配置され、マーカーを写す。</paragraph></section><section ids="id22" names="実験の目的"><title>実験の目的</title><paragraph>上記のようなパノラマを用いた共同作業システムには、いくつかの根本的に不明瞭な点がある。まず、複合現実感を用いたシステムの中でさらに映像合成を行っているため、システムについての理解や、システムを通じた視点の理解がスムーズに行われるのかという問題がある。これはいわゆるユーザビリティに当たる(できれば定量評価でだめな理由)。また、本システムは簡潔で、基礎技術的な位置づけである。これを共同作業に適したシステムにするために、基礎的な技術のみを用いたインタラクションについて理解することが有用である。主にこの2つを目的とする。</paragraph></section><section ids="id23" names="実験の概要"><title>実験の概要</title><paragraph>本実験では、ミニチュアの家具を配置するタスクを、指示者、配置者の2名の共同作業によって行った。指示者は家具の配置の写真を見ることができるほか、技術的手段によって設定によっては配置の様子を見ることができる。配置者の前には家具配置スペース(紙によって示されている)と、ばらばらに置かれた家具がある。指示者と配置者は同じ部屋にいるが、お互いを見られないように配置されており、肉声によって会話をしながら家具の配置作業を行う。</paragraph><paragraph>指示者の環境設定は、目の前に表示用のPC(MacBook Pro 13inch Early 2009)があり、映像やパノラマ映像が表示される。また、写真表示用のデジタル一眼カメラ(Panasonic DMC-G1)やiPhone 3GS(パノラマ実験ではデジタル一眼カメラがシステムに利用されたためこちらを利用)があり、それぞれ基本的な操作によって写真の閲覧や拡大縮小が可能である。パノラマ実験の場合は、この他にパノラマ操作用にマーカーとマーカー認識用のデジタル一眼カメラが配置されているが、配置は途中で変更した。</paragraph><paragraph>配置者の環境設定は、目の前に2つの机があり、手前と奥に配置されている。手前の机では配置するためのA4の用紙や、パノラマ実験の場合は中央にパノラマ用のカメラが配置されている。奥の机には、あらかじめミニチュアの家具がバラバラに置いてある。</paragraph><paragraph>実験手順を以下に示す。</paragraph><bullet_list bullet="*"><list_item><paragraph>前の配置を利用しない場合、ミニチュア家具を配置する</paragraph></list_item><list_item><paragraph>ミニチュア家具の配置の写真を撮影する</paragraph></list_item><list_item><paragraph>ミニチュア家具をバラバラに奥の机に置く</paragraph></list_item><list_item><paragraph>被験者に実験について説明する</paragraph></list_item><list_item><paragraph>実験と撮影を開始する</paragraph></list_item><list_item><paragraph>指示者と配置者が共同してミニチュア家具を配置する</paragraph></list_item><list_item><paragraph>指示者が終わりだと宣言した場合、実験、撮影を終了する</paragraph></list_item></bullet_list><paragraph>実験は、以下の3つの技術設定で行った。</paragraph><bullet_list bullet="*"><list_item><paragraph>音声のみ:指示者は配置を真上から撮影した写真のみを見ることができ、配置者の状況は会話によってしかわからない。</paragraph></list_item><list_item><paragraph>映像:指示者は写真の他に、配置者を斜め上から撮影した映像(カメラ1をそのまま表示したもの)を見ることができる。</paragraph></list_item><list_item><paragraph>パノラマ映像:指示者は写真の他に、家具配置スペースの中央から撮影したパノラマ映像を、前節で説明したパノラマ映像表示装置によって見ることができる。</paragraph></list_item></bullet_list><paragraph>以下に、個別の実験の詳細についてまとめた。</paragraph><table><tgroup cols="1"><colspec colwidth="8"/><tbody><row><entry/></row><row><entry><paragraph>技術設定</paragraph></entry></row><row><entry><paragraph>指示者</paragraph></entry></row><row><entry><paragraph>配置者</paragraph></entry></row><row><entry><paragraph>使用写真</paragraph></entry></row><row><entry><paragraph>カメラ1</paragraph></entry></row><row><entry><paragraph>カメラ2</paragraph></entry></row></tbody></tgroup></table><paragraph>ただし、3,4,5はそれぞれ実験5,6,7の結果を撮影したものである。</paragraph><section dupnames="これによって何がわかったのか？" ids="id24"><title>これによって何がわかったのか？</title><paragraph>まず前提として挙げておきたいのが、このシステムは元々一つの部屋を領域として、本物の家具と同じ程度の物体を配置することを目的として設計されており、ミニチュアの家具を用いた実験を行ったのは、あくまでそれを擬似的に再現したものであるということである。この場合、「映像を用いた実験」のような設定を行うことは難しくなる。映像を用いた実験では、ミニチュアの家具よりかなり高い場所にカメラが配置され、全体を俯瞰できるようになっている。しかし、実際に部屋にこのようなカメラを配置することは物理的に難しく、例えば監視カメラのような配置だと死角ができるだろう。このため、もし「パノラマを用いた実験」が「映像を用いた実験」より何らかの劣った面があったとしても、それは必ずしもパノラマシステムが劣っていることを意味しない。</paragraph><paragraph>また、この実験をミニチュアで行うことが、実際の部屋で家具を配置することと異なる特徴を持つ可能性がありうる。しかし、パノラマ表示インタフェースに関しては、ミニチュア家具、展示会場、都市空間で特に特性が変わらないことを確認している(以下の写真を参照)。あまりに小さすぎる場合だと焦点距離の問題で像がぼやけてしまうが、今回の実験はA4の用紙を配置場所として選択しており、パノラマの周囲4cm(カメラの接近できる限界)には物体が配置されていない。</paragraph></section></section></section><section ids="results" names="results"><title>Results</title></section><section ids="conclusion" names="conclusion"><title>Conclusion</title><citation backrefs="id8" ids="garfinkel1967" names="garfinkel1967"><label>Garfinkel1967</label><paragraph>Garfinkel, H.,1967, &quot;Studies in Ethnomethodology&quot;, Prentice-Hall</paragraph></citation><citation ids="randall2007" names="randall2007"><label>Randall2007</label><paragraph>Randall, D., et al., 2007, &quot;Fieldwork for Design&quot;, Springer</paragraph></citation><citation ids="button2009" names="button2009"><label>Button2009</label><paragraph>Button, G., Sharrock, W., 2009, &quot;Studies of Work and the Workplace in HCI&quot;, Morgan &amp; Claypool</paragraph></citation><citation ids="schegloff2007" names="schegloff2007"><label>Schegloff2007</label><paragraph>Schegloff, E., A., 2007, &quot;Sequence Organization in Interaction: A Primer in Conversation Analysis I&quot;, Cambridge University Press</paragraph></citation><citation ids="suchman2007" names="suchman2007"><label>Suchman2007</label><paragraph>Suchman, L., 2006, &quot;Human-Machine Configuration: Plan and Situated Action 2nd Edition&quot;, Cambridge University Press</paragraph></citation><comment xml:space="preserve">[]</comment><rubric>註</rubric><footnote auto="1" ids="id25" names="1"><label>1</label><paragraph>別の手法として、概念分析などがあるがここでは触れない。</paragraph></footnote></section></document>