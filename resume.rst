==========================================================
修士論文「位置提示技術を用いた状況での相互行為の分析」要約
==========================================================

(第一部)

背景と目的
============
(要出典)(分厚く)本論文では、実世界の環境の提示の中でも、空間に関するものが、離れた環境でどのように用いられるかを、相互行為分析の手法を用いて分析した。この背景には、コンシューマコンピューティングの技術の進展により、新たな遠隔地での共同作業の可能性が開かれたことがある。現在、SOHOやノマドワーキングなどの、特定の組織や場所に縛られない働き方が提唱されている。一方で、このような働き方ができるような業種は、いわゆるホワイトカラーや、ソフトウェア開発など一部に限られる。これは、現在普及しているPCのインタフェースがオフィスワークを目的としたものであることに大きく起因すると考えられる。画像処理、拡張現実感技術や、「タッチ」を中心としたタンジブルなインタフェースが安価に手に入りつつあるが、これを実用的に応用する試みはまだ少ない。本論文では、この可能性を模索することを目的とする。

本論文の構成
------------
本論文では、エスノメソドロジー、相互行為分析のシステムデザインへの適用が主軸に置かれている。しかし、これ自体が主題ではなく、あくまで技術的環境の変化がこのような手法への注目や重要性を喚起している。よって、本論はまず、昨今の技術的環境の変化を概観しながら(1,2章)、エスノメソドロジーとシステムデザインとの関係についてのレビューを行い(3,4章)、その実際例として著者が関わったフィールドワークと実験について取り扱う(5,6章)。これらを便宜上第1部から第3部とする。

技術的概観
=============
本論文では、コンピュータとのインタフェースの中でも、実際の空間で手で触ることができるようなものや、実際の空間から何らかの形で情報を得るようなものを取り扱う。実際の空間を指向したような技術は、情報分野では複数の研究分野から生まれ、互いに影響しあって形成されている。このため、似たように見える技術コンセプトでも、実世界での作業を支援する側面、あるいはテーブルトップでの作業を拡張したもの、空間の境界自体をインタフェースにしたものなど、多くのバリエーションや、また先端の分野で忘れられた概念なども存在する。このため、まず実空間を志向した様々な分野のレビューを行い、概念史をまとめる。

実世界以前
---------
本論文では、バーチャルリアリティや実世界を志向したインタフェースに関連する分野について取り扱う。その中で重要なのが「実世界」とは何かということである。例えば実世界を、見て触ることのできる世界と定義できるだろう。しかし、現存するパーソナルコンピュータなどは明らかに見て触ることができるものの、実世界を志向したものとして扱われることはほとんどない。この問題を明確にし、バーチャルリアリティや実世界志向インタフェースが何を実現するのかについて検討するため、まずは現在のパーソナルコンピュータや、実世界を指向する以前の試みについて少しまとめる。

GUI
~~~

パーソナルコンピュータのユーザインタフェースの類型として現在典型的なものが、GUI(Graphical User Interface)である。例えば、本論文の執筆環境は、LinuxのCompizというオーソドックスなGUIに、論文本体の編集画面、文献をまとめたファイルブラウザ、文献を表示するドキュメントビューワーの3つのウインドウからなる。これらをタッチパッドのクリックによって切り替えることができ、タッチパッドの右端を上下になぞると文献のページをめくることができる。

GUIによるコンピュータの操作を可能にする概念は、椎尾によって以下のようにまとめられている。

* 直接操作:コンピュータ画面に表示した物体を、ユーザが指示装置で動かすことで、コンピュータを操作する手法。ユーザは物理的な物体を操作している錯覚を覚え、わかりやすいインタフェースを実現できる。また、状態を直感的に見て操作できるため、コンピュータの動作を把握、支配している感覚を得られる。物を操作する能力は幼児期に獲得するものであるため、緊張感や負担が少なく、知的作業の妨げになりにくい。
* メタファー:GUIでは、コンピュータ画面を事務机の上(デスクトップ)に喩えた、デスクトップメタファー(desktop metaphor)が採用されて( [Siio2010]_ , p.108)いる。つまり、書類、フォルダ、アプリケーションなどが実際の机を模してアイコンとして表示されている。メタファーはコンピュータの機能の理解と学習を容易にし、現実世界の知識を利用できる。ただし、現実とかけはなれた挙動をする場合はこの限りではない。
* WYSIWIG:What you see is what you getの略。最終的に出力される文書や図版と、見かけ上全く同じものをディスプレイに表示し、操作することができる。
* やりなおし:GUIでは操作の手がかりが多く表示されているため、ユーザーは試行錯誤によって操作を習得する。このためにはやり直しができる機構が必要である。
* モード:コンピュータシステムの状態によって、ユーザーが行う操作の意味が変わったり、実行できる操作に制限がかかるインタフェース( [Siio2010]_ , p.111)を、モーダルなインタフェース、そうでないものをモードレスなインタフェースと言う。モードはユーザーの作業の妨げになるため、モードレスが推奨されるが、作業を中断してでも通知、確認する重大な場面ではモードが使われる。
* GUI設計のガイドライン:複数のアプリケーションで統一した操作を提供すれば、ユーザが操作方法を学習する負担を削減できる。そのために設計のガイドラインと、共通して使える部品を開発者に提供する必要がある。

以上 [Siio2010]_ , pp.107-115より

このうち、「直接操作」と「メタファー」はGUI全体に関わる事柄である。Hutchinsらは、直接操作を2つのグラフ描画の例を挙げてこう記述している。

【Shneidermanの論文がないためパスか保留】
【メタファーに関しては、DigitakDesk論文に参考文献あり】

この2つの重要性は開発者にも同様に認知されている。最初に普及したGUIを搭載したコンピュータであるAppleのMacintoshでは、「Macintosh Human Interface Guidelines」という書籍を開発者向けに出版している。その中の「ヒューマンインタフェースの基礎」という章では、メタファー、直接操作、WYSIWIG、一貫した操作、モードなどについて取り上げている。これは、以上の概念が単なる設計思想に留まらないことを示している。


この2つは、一見して身体的な「タッチ」という操作を実現しているAppleの「iPhone」にも一貫して重要視されている。iPhoneのアプリケーションを開発者が提供する際にはAppleの審査を受ける必要があるが、その中で最も重要な審査基準である「iOS Human Interface Guidelines」では、タッチ機能は直接操作やメタファーをさらに補強するものと位置づけている。

  iOSのユーザーはマルチタッチのため、直接操作の強い感覚を楽しむことができる。ジェスチャーの利用は、画面上の見ている物体に大きな親近感と、それを制御している感覚を与える。(p.20)
  人々は現実的な画面上の物体と物理的にインタラクションを行い、多くの場合現実世界の物体に対するかのように操作できる。(p.21)

iPhoneでは触るという操作は、あくまで画面上のメタファーに対して行われる。これは明らかにGUIの延長線上にある製品である。

初期のバーチャルリアリティ
~~~~~~~~~~~~~~~~~~~~~~~~~


CSCW
----
一方で、実世界を重視するという流れには、CSCW(コンピュータ支援共同作業)分野の一連の研究が深く関わっている。CSCWは、一言で言えば、実際のオフィスワークを念頭において、それをコンピュータによって支援する研究を総称する。その中には、例えばスケジュール管理に代表される「グループウェア」も当然重要な位置を占めるが、実際のオフィスについての分析を行う、もしくは実際のオフィスに適合するようなインタフェースの研究は、前節の実世界志向インタフェースにも接続されている。

机の拡張
--------
机上の共同作業を支援する試み、特に紙の文書とデジタル文書をシームレスに扱うようなモデルは、複数の研究者によって提唱されており、後の実世界を志向したインタフェースや、映像を用いたコミュニケーションへの礎となっていった。本章でのレビューはこれを出発点とする。

TeamWorkStation
~~~~~~~~~~~~~~~
石井らによるTeamWorkStation(TWS)は、デスクトップ画面をビデオ制御することで作業領域の共有を可能にするシステムである。これは単純な概念であるが、拡張現実感に至る出発点であった。TWSのキー概念は以下のようになる。

* ホワイトボードのような「共有描画表面」があり、全員が見て差し、描くことができる
* 共有作業領域と個人作業領域の間をシームレスに移行できる

これらを実現するために、「個人作業領域をオーバーレイする」ようにTWSは設計されている。つまり、例えばAの画面にBの画面が表示され、Bの画面をAがマウスポインタで指すことで、それがフィードバックされAの画面にも現れるような設計になっている。この他にも様々なオーバーレイの形式が提示されている。それだけではなく。基本的にビデオ媒体を利用しているため、ビデオカメラ映像のオーバーレイも可能である。これにより、実際の机や顔なども共有することができ、デスクトップに留まらない「作業領域」の共有が可能になる。

DigitalDesk
~~~~~~~~~~~
紙の文書とデジタル文書における作業の統合を目指したのがWellnerによる、「DigitalDesk」である。Wellnerは、紙とデジタルの文書が分離されている状況を「dual desk」として描写している。紙文書とデジタル文書は別の機能を持ち、媒体の違いから相互に変換することも難しい。この両者をうまく統合させる方法が「Computer Augmented Environments」だとWellnerは提唱する。

Computer Augmented Environmentはバーチャルリアリティ(VR)に着想を得ているが、逆のアプローチを取っている。VRはコンピュータの作り出した世界に仮想的な物体を配置することができ、それは日常生活を支援するのに有用であるが、実際の世界から遮断されてしまう。これに対してComputer Augmented Environmentは実世界の物体をコンピュータによって拡張することを目指す。これによって物理的環境の慣れ親しんだ特徴を失わずに、コンピュータの支援を受けることができる。この考えはユビキタスコンピューティングと拡張現実感ともつながっている。

DigitalDeskでは、実際の机の上にプロジェクターによりコンピュータ画面を投影し、またカメラで机の画面を撮影することで、実際の机上の紙をコンピュータで処理することを可能にしている。これによって、メタファーではなく実際のペンや指での操作が可能になる。また、紙の上にコンピュータの画像をオーバーラップさせたり、紙文書を認識してスキャンすることもできる。DigitalDeskのインタラクションは、指を用いた現実、仮想の物体との「tactile interaction」を目指す。つまり、紙とデジタルデータの両方に同じ方法で接することができる。

ClearBoard
~~~~~~~~~~
石井らによる「ClearBoard」は、以上のような「ホワイトボード的」な共同作業支援システムが、視線やジェスチャーの問題を持っていることから考案されたシステムである。TeamWorkStationの実験から、対面会話から共有描画活動にスムーズに移行することが重要であるという結果が出た。

対面の会議では「隣接した空間」、つまりホワイトボードと人の間に物理的な継ぎ目がないものとして部屋が知覚され、目や頭を動かすだけで参加者とホワイトボードを見渡せる。しかし、TWSでは分かれて扱われ、仮想的な会議空間が分離してしまった。この問題に対処するため、シームレスな共有作業空間と、アイコンタクトに焦点を当てた2人用の遠隔リアルタイムコラボレーションシステムが、ClearBoardである。

ClearBoardではシームレスでアイコンタクトのある空間を実現するために、「ホワイトボードの前にいて」「テーブルの向こうにいる」、そして「通り抜けて話し、描けるような透明なガラスのウインドウ」の3つのメタファーが用いられる。この3つにより、まず仲間の顔を見ることができ、次にあまり目を動かすことなく、仲間の顔と描画の間で視線を動かすことができる。

実世界志向インタラクション
--------------------------
CSCWにおける、机などのオフィスの実際の物体に即したインタフェースの試み、もしくはオフィスワーク以外の現実世界での活動の支援の試みは、現実世界に十分適合したインタフェースの研究へと継承されることになった。このテーマはそれ自体が独立したものであり、実際に現在研究されている実世界を指向したインタフェースが、必ずしも共同作業の支援という問題意識につながっているわけではない。本節では、実世界を指向したインタフェースの初期の概念について整理し、現状についてまとめる。

Augmented Reality/Mixed Reality
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
実世界を指向したインタフェースの一つの方向性が、拡張現実感(Mixed Reality)、拡張現実感(Augmented Reality)と呼ばれるものである。この2つの概念は互いに重複することも多いため、本節では同じものとして取り扱う。これは、単純に要約すれば現実世界の物体や空間と、コンピュータが作り出した知覚を重ね合わせるという概念であるが、2つのまったく異なる起源を持つ。

その一つが、前節で触れた「Computer Augmeneted Environments」やClearBoardをはじめとするテーブルトップ環境である。既に見てきたように、共有作業空間と、実際の人間の視線や指差しなどのを両立させる試みは、仮想的なデータ表示と実際の映像を重ねるデザインに至った。

こちらの拡張現実感のイメージは、ユビキタスコンピューティングに近い。Buxtonは、ユビキタスメディアは拡張現実感であるとする。

もう一つが、バーチャルリアリティ研究から派生した、シースルー型HMDなどの利用の研究である。

実世界志向インタフェース
~~~~~~~~~~~~~~~~~~~~~~~~~
前の節で見たように、基本的にコンピュータや携帯電話のインタフェースはGUIの延長線上にある。しかし、GUIの問題点やコンピュータを取り巻く環境の変化を元に、新たなインタフェースが幾つか生み出されている。それらはGUIのような一つの概念ではないが、相互に影響しながら研究が行われてきた。

実世界志向インタフェース [Rekimoto1996]_ は、その中でも「実世界での人間の作業を支援しようという研究の流れ( [Rekimoto1996]_ , p.2 )」という広い範囲を取り扱う概念である。暦本は、実世界志向インタフェースの特徴を以下のように要約している。

* インタフェースの透明化:利用者のタスクは実世界のもので、実世界に注意を向けているため、システムに注意を集中させることはできない。このため、メタファーのように「見せる」方向ではなく「透明にする」方向が問題となる。究極的には人間がコンピュータを認識しなくなる。
* 実世界状況の認識:実世界のタスクを支援するためには、利用者が実世界で置かれている状況や意図をコンピュータが認識する必要がある。このため、コンピュータには状況を認識して積極的に情報を提供するような能動性が求められる。
* 人間の能力の強化:実世界志向インタフェースの目標は、人間の代わりではなく人間の能力そのものを擬似的に増強することが一つである。
* 実世界情報とコンピュータ情報の関係:現実世界の情報とコンピュータの情報をいかに連携させるかが重要なテーマである。これにはいくつかの種類がある(図)。(a)(左上)従来型インタフェース。コンピュータと対面する。実世界のインタラクションとの間にギャップがある。(b)(右上)仮想現実感。完全にコンピュータの作り出す世界に限定され、現実世界とのインタラクションはなくなる。(c)遍在型コンピュータによる実世界志向インタフェース。コンピュータを遍在させることで実世界と仮想世界を一体にする。(d)携帯型コンピュータによる実世界志向インタフェース。cは現実を、dは人間を強化するアプローチといえる。

.. figure:: interactionstyle.eps
   :scale: 50 %

   インタラクションスタイルの比較( [Rekimoto1996] より著者が作成)

タンジブルインタフェース
~~~~~~~~~~~~~~~~~~~~~~~~
MIT Media Lab. の石井が提唱した「タンジブルユーザーインタフェース」(TUI)も、GUIの代わりとして実世界を志向したインタフェースの一つである。

TUIは、97年に4つの学問領域の影響を受けて、建築空間に関するものとして提示された [Ishii1997]_ 。1つめがWeiserの提唱するユビキタスコンピューティングである。これについては詳しくは取り扱わないが、コンピュータが「透明」になり、遍在化することを予測している。この結果、物や建築の表面にコンピュータが埋め込まれる。次が、先に取り上げたAugmented Realityで、物を直接つかみ操作するという焦点を導入した。次がCSCWである。机上の遠隔仮想空間の提示(ClearBoard)は、表面を仮想空間と現実空間の間で、情報が自由に行き来するアクティブ・インタフェースにするという着想を与えた。最後がGrapsable User Interfaceである。これも情報を直接手でつかむという着想を与えた。

以上のような領域に受けて、「Tangible Bits」という一連の研究のコンセプトが提示された。

* インタラクティブな表面:建築世界の表面を物理世界とディジタル世界のインタフェースとする
* ビットとアトムとの結合:手で操作できる物理オブジェクトとディジタル情報をリンクできる
* アンビエント・メディア:建築空間内の音や空気などを、サイバースペースとのバックグラウンドインタフェースとして利用する

以上により、ディジタル情報を認知の焦点でビットを直接つかんで操作でき、また認知の周縁で情報の気配にアウェアでいられるようにすることを目指す。つまり、GUIでコンピュータに焦点を当てていたものをより現実世界とスムーズにすることを目指している。

近年の研究では、その概念はさらに具体的になっている。TUIは、人の物理的環境を感知して操作する能力を活用するため、デジタル情報を物理的空間で物理的に身体化された形で扱うものである( [Ishii2008]_, p.470)。GUIはディスプレイ上のピクセルとして情報を表すが、それとのインタラクションは我々が生活する物理的環境と不整合であり、物理的な物体を扱う能力を十分に発揮できない。TUIはデジタル情報に物理的な形を与えることを基礎とし、デジタル情報を手で「直接操作」することを可能にする。しかし、TUIは特定の目的のために特定の物理的形状を与えるもので、GUIのようにあらゆる目的にかなうものではない。

TUIの基本的なモデルには、GUIと共通する部分と異なる部分がある。TUIは、GUIと同じようにMVC(Model-View-Controller)という設計モデルを採用している。これは、データの取扱いを決める「モデル」、情報の提示を管理する「ビュー」、プログラム全体の制御をする「コントローラー」の3つにプログラムの部品を分ける手法で、近年のWebアプリケーションなどにも採用されている。TUIでは、コントローラーはタンジブルなものを扱うものと、そうでないものに分かれる。また、モデルは「デジタル情報」として一般化される。

TUIはGUIと同じく、デジタル情報の直接操作を行うが、タンジブルな表象を提示する。タンジブルな表象は物理的世界との架け橋となるとともに、デジタル情報と計算モデルの制御を可能にするように計算論的に結合されている。つまり、手などによる物理的な操作による位置などのパラメータが、制御に利用されている。一方で、TUIには物理的な制約があるため、映像投影や音声などの「インタンジブルな」インタフェースも補完的に使われる。

TUIの基本的な特徴には以下のようなものがある。



補論:Jointed Reality
~~~~~~~~~~~~~~~~~~~~
2009年5月、著者は「実際に何か実世界に関連したもの」を製作し、それを評価することで研究を進めることを計画していた。その際、「モバイルデバイスで空間を取り扱う」ことをコンセプトに、「Jointed Reality」(これはネーミングが先行している。「コンピュータビジョン・拡張現実感に関する普通じゃない勉強会」というセミフォーマルな発表会において、発表内容に「VR」「AR」に変わる「*R」を表す名称を付けるという条件が課された。その際国鉄になぞらえた名称である)というコンセプトを発表した。それが、タンジブルインタフェースの「表面にインタフェースが埋め込まれる」という当初のコンセプトに関連していると思われるので、ここで取り上げる。

「Jointed Reality」は、元々表と裏に液晶を持つモバイル端末の使用法について検討している中から生まれた。当初検討していた2つの液晶を持つ利点は、モードを直観的に切り替えられることである。つまり、表と裏に関連した別の機能を割り当て、それを回転させて切り替えることで直観的で豊富な機能を扱えるというコンセプトである。また、巻物のメタファを導入し、回転させると次のページが現れるようなインタフェースも試作した。しかし、これらは現状のGUIに対して大きな利点とならないと推測された。

次に著者が検討したのが、モバイル端末の空間性の利用である。モバイル端末には、当然幅、高さ、奥行きが存在する。ある意味で、それは常に一定の空間を占有していると言える。この空間とバーチャル3D空間をマッピングし、何らかの形で操作を与えることで、平面ではなく空間を扱えるようなインタフェースが可能になると考えられる。

具体的なインタラクションの形式は、基本的に「タッチ」操作の応用となる。試作した両面液晶端末は、表と裏にMID(Mobile Internet Device)を貼り付け相互に通信を行い、表裏判別用にWiiリモコンが横に貼り付いているという構造である。当時はMIDに本格的な3D機能がなかったため、拡大縮小回転による擬似3D機能を用いて、インタラクションのデモを作成した。その中には、両面の液晶をつまんで物体を移動できる「つまむ」機能、両面の液晶で別の方向に指を動かすことで物体を回転させる「まわす」機能などが含まれる。このように、2つのタッチ液晶を「Joint」させることで、その中に3D空間を発生させるというコンセプトが「Jointed Reality」である。

これは、元々2D+GUIのシステムをTUIとして再提示した例と言える。【もう少し前の部分をまとめてから書く】


現況
~~~~

よく知られている製品の例が、ARToolKitとセカイカメラである。ARToolKitは、「マーカー」という、コンピュータが認識しやすい模様を用い、それが映像の中で認識された場合、その場所を基準として3Dの物体を表示するものである。セカイカメラは、iPhoneのアプリケーションで、主にGPSや加速度センサーなどの情報を元に、カメラ映像の上に文字などが書き込まれた吹き出しを表示させ、あたかも吹き出しが実世界にあるかのように見せるものである。

コンシューマ領域での実世界志向技術の現況は、どのような技術がコモディティになっているかによってある程度知ることができるだろう。

消えるコンピュータと人間の拡張
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
以上の「ポストGUI/実世界志向」アプローチに共通する点は、「コンピュータを見えないものにし」「人間の能力を拡張する」ことである。この2つの主張は一見して独立したものに見える。しかし、これらはある意味で共通した面を持ち、その共通点を見ることこそが実世界志向インタフェースの別の側面を明らかにする。

例えばARToolKitについて、

* カメラで認識した映像フレームの中の特徴点を元に3次元位置の推定を行い、それを基準として3Dモデルとカメラの映像を重ねてディスプレイに表示する
* 現実世界に置かれたマーカーの上に、3Dモデルが配置されることで、あたかも仮想的な物体がそこにあるかのように見える

以上のような2種類の記述を行うことができる。前者は画像処理の結果をディスプレイに表示している点で、GUIに属するものと見られる。一方、後者は実世界に仮想的な物体を提示するARシステムである。しかし、この2つは全く同じシステムである。また、この2つは「実世界で使われているから」あるいは「使い方が異なる」という理由で異なっているのではない。使われる状況や使い方に依存しない記述である。

システム自体ではなく、この記述からARToolKitが実世界を志向している、つまり「コンピュータを見えないものにし」「人間の能力を拡張する」ことを示す。まず、「カメラ」「ウインドウ」は後者では消えているため、コンピュータは確かになくなっている。また、前者ではただ表示しているだけだが、後者では仮想的な物体を見ることができるようになっているため、例えば有用な物体を表示することを考えれば人間の能力は拡張されていると言える。

以上のように、コンピュータによる人間の能力の増強を考える際には、達成されたものがコンピュータの能力に属するか、もしくは人間かという帰属の問題が起こる。これに関しては、人間とコンピュータの境界の問題としてSuchmanが論じている( [Suchman_2006]_ )。

まとめと問題設定
~~~~~~~~~~~~~~~~
以上のように、実世界を志向したインタラクションという技術的な挑戦は、その初期においては人間同士の共同作業という、比較的純粋な技術から離れた分野と密接な関わりを持っていた。その接点には、人間同士の机上、オフィス空間などでのインタラクションを、可能な限り円滑な形でシステムに取り入れようというモチベーションがある。「実世界」というものを志向する意義は、人間がそこにいてこそのものであるといえる。

一方で、幾つかの本質的な疑問が残る。例えば、共同作業システムの完成形として、人間同士のインタラクションで必要な要素のみを提示できる、という状況を考えれば、共同作業の支援に「世界」を提示するというのは冗長である。また、幾つかの研究では「視線や身体的動きなどが行われる空間」を「実世界」と呼んでいる。一方、先に見た例では、人間とシステムの境界そのものが曖昧である。それに応じて「視線や身体的動きなどが行われる空間」も変わってくるはずである。このように、「実世界」という言葉の定義は不明瞭である。既にそのような「実世界」を志向したシステムがコンシューマー領域に入りつつある。

以上から、本論文で扱うテーマをある程度確定することができる。つまり、「実世界を志向したインタフェースは、実世界をどのように扱うのか」である。これに対して、工学的な観点のみから問題をとらえることは難しい。接近する手段があるとすれば、

* 人間がインタラクションの中でどう「実世界」を形成するのか
* 実世界を志向したインタフェースの導入で、どう「実世界」が変わるのか

を通じてである。本研究では、この2つに焦点を当てて、「実世界を志向したシステム」についての解明を目指し、いくつかのフィールド調査や実験を行った。

実際の事例に入る前に、次章から人間同士のインタラクションを分析する手法について検討する。

(第二部)

分析の方法論と方針
======================

以上で取り上げたような技術は、高度に環境に依存し、即時的な特徴を持つ。このため、主要な問題もHCIでメジャーな分析手法である行動科学に基づく手法では、不十分であるかずれていることが考えられる。そこで、本論文ではエスノメソドロジーに基づく相互行為分析の適用を試行する。これは、ビデオによって、その場に居合わせた人々の行為がどのように組織化されているかを記述する手法で、CSCWなどの分野でも比較的応用が多い。以下ではエスノメソドロジーと主要な手法である会話分析について、少なくとも著者の理解を示し、それを元に相互行為分析の特徴と可能性について説明する。

以降の議論では、主に分析の方法について取り扱うが、社会学の分析手法と、システムデザインの目的、手法、アウトプットなどを混在して扱うことになるため、それらが錯綜してしまいがちである。つまり、

* エスノメソドロジーは何に焦点を置き、どうやってそれを分析し、それによって何を得るのか
* システムのコンセプトはどう決定され、どう作って、どうちゃんと作られているかを評価するのか

という2種類の異なる立場から、少なくとも分析を行う立場において以下のようなことを決定しなければならない。

* システムのデザインという目的設定の元で、エスノメソドロジーをどう行い、何を得るのか

本章ではこの3点について、それぞれを検討することによって、エスノメソドロジーによるシステムが関わる状況の分析について明かにする。なお、ここでは主に分析を行う側にのみ焦点を当てるが、分析側とデザイン側が共同で作業を行うことの問題については次章で検討する。

概要
----
エスノメソドロジーは、単に日常生活を研究するのではなく、それが既に秩序だっているような手続きを研究する分野である。これを実際に記述する手法が会話分析や相互行為分析で、これらは相互行為のシークエンス的な組織化を詳細に明らかにする。これは、その場面である作業を達成するために、どのようにその場その場で成立する秩序を成員が理解し、次の相互行為につなげているかということがわかる。

エスノメソドロジー
------------------
(この辺から再構築する)
エスノメソドロジーは、創始者のHarold Garfinkelによって以下のように特徴づけられている。「私が「エスノメソドロジー」という言葉を使う際は、日常生活の組織立った巧妙な実践の、偶発的で継続的な達成としての、文脈指標的表現やその他の実践的行為の規範的特徴の研究を指す」([Garfinkel1967]_, p.11)。つまり、我々が何かの枠組みをもって行為を説明する以前に、人々の実践的行為はすでに秩序立っている。この秩序を解明することが、エスノメソドロジーの最も基本となる考え方である。とはいえ、エスノメソドロジーは、単に人々の日常を明らかにする、ということではない。(説明可能性と、できれば文脈指標性の議論)

この議論では、具体的にどう明らかにするのか、というところまでは踏み込んでいない。エスノメソドロジーを具体的にどうやっていくのかということに関しては、当時エスノメソドロジーが大きな影響を与え、またその代表的な研究手法となった会話分析について触れる [#]_ 。会話分析は、主にSacks, Schegloff, Jeffersonらによって開始された、会話の組織化に関する広範な研究である。会話分析の対象は近年 [Schegloff2007]_ (ページ洗い出し)によって以下のように特徴づけられている。

* 順番交代 (turn-taking) 問題:会話において誰が次に話すのか?またそれはいつ行われるのか?
* 行為形成 (action-formation) 問題:どのように、言語、身体や、相互行為の環境、相互行為内の位置などのリソースが、設計された通りの構造に、また受け手に、その規模もわからないのに特定の行為 (例えば、依頼、招待、許可、不平、同意、知らせ、警告、拒絶など) として認識されるように形成されるのか?
* シークエンス組織 (sequence-organazational) 問題:どのように、次の順番が前の順番と「筋の通った」ものとして形成されるのか?また、そもそも「筋が通った」の本質とは何か?
* トラブル (trouble) 問題:どのように話し、聞いたり、会話や相互行為を理解する際のトラブルが、それが起こった際に止まらず、間主観性が維持、修復され、順番やシークエンス、活動が可能な完了へと進むように扱われるのか?
* 言葉の選択 (word-selection) 問題:どのように順番の単位となる構成要素が選択されるのか?また、どのようにその選択が、受け手が理解を達成できるように知らせ、形成されるのか?
* 全体構造の組織化 (overall structural organization) 問題:相互行為の出来事の全体的な組織は、どのように組み立てられるのか?その構造とは何か?また、どのように全体構造の配置が、その構造と、シークエンスや順番としての会話を知らせるのか?

会話分析においては、会話の録音と、それを文字に起こして分析を容易にするトランスクリプトが分析の基礎になる。先駆的な研究によって、会話の組織化には発話の間や複数の発話のオーバーラップなどが有意であるということが明らかになっている。これらを含めて書き起こせるようにしたのが、Jefferson Systemであり、後の相互行為分析に使われるトランスクリプトでもその拡張が使われている。特有の記号などについては実際の分析で必要なものをその都度説明する。

(再構築前)

相互行為分析
------------

「相互行為分析」は、主にGoodwin, Heathらによって始められた、会話も含めた身体的相互行為をビデオによって分析する方法である。対面した相互行為では、会話の書き出しだけでは発話のポーズなどを説明できない場合がある。もしくは、会話がなくても何らかの相互行為を組織させる、ということはよくあることである。相互行為分析は、前述の会話分析の拡張ではあるが、環境、指示などのあり方にさらに迫ることができる。

相互行為分析が明らかにした知見
--------------------------------

Goodwin, Heath/Luffなどの「CSCW以前の」成果(流れの都合)

エスノメソドロジーとCSCW
------------------------

エスノメソドロジーが貢献しうる役割
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
エスノメソドロジーによる共同作業システムの分析がどのような役割を果たすかに関しては、いくつかの見解がある。これは後述するデザインプロセスの問題にも関連している。

Buttonによるまとめ([Button2009]_, pp.39-43)では、エスノメソドロジーのワーク研究が設計の目的に対して使われる際には、4種類の使い道があるとしている( [Button2009]_ , p.39)

* 批判:既存の設計手法で作られたワークフローシステムは、実際の場面に導入された場合に、詳細な分析をした際に明らかになるような、作業の組織化の状況に埋め込まれており即時的な特徴のために困難に直面してしまうということを示すために用いられる
* 評価:特定の技術デザインを評価するために用いられる。実際のワークプレイスにシステムを導入した際に得られたデータを分析し、システムの改善に活かす。
* 要求:実際のワークプレイスを分析して得られたデータを元に、システムの要求を決める。 Bentley1992 によれば、ワークプレイスの分析は要求を詳細に定義するのにはあまり有用ではないが、設計の際の適切な意思決定を提供する。
* 基礎的な関係:設計者とワークプレイスの分析者

(このほか、 [Randall2007]_ の5章、6章前半の議論)


具体的な成果例
~~~~~~~~~~~~~~

(Heath/Luff)

(Mixed Reality Labの一連のmixed reality関連の調査、Benford, Rodden, Crabtreeなど)

(Brownらの地図に関する研究)

(Kirkらのテーブルトップの実験)

( [Randall2007]_ の8章など)

システムデザインへの適用の問題
==================================

相互行為分析などの、エスノメソドロジーに影響を受けた手法(Ethnomethodology-informed Ethnographyや、会話分析なども含む)をどう実際のシステム設計に取り入れるかに関しては、その当初から議論が存在する。前章ではシステムが関わる状況でのエスノメソドロジーについて検討したが、分析のアウトプットは必ずしも設計者の関心の中にないかもしれない。例えば、あるタスクを行わせて各段階での作業時間を計測することは、システムの評価に有用だろう。また、新たなシステムを設計するために以前のシステムについてインタビューを行ったり、SD法によって感性を調査することは、少なくとも筋が通っている。しかし、エスノメソドロジーや相互行為分析に関しては、前章で見てきたように、単純に「実際の環境での使用を見る」「日常生活について理解する」などの視点で見ることができない。何より、分析結果が単純に何が良い悪いということを必ずしも提示しない。

そのような前提を元に、エスノメソドロジー的調査はどう行えばよいのだろうか。その中には、完全に設計を無視して行う方法から、設計の際に必要なことだけを集中的に分析する方法まで多様な可能性があり得る。また、それに応じて分析の設計に対する位置づけも変わってくる。本章では、エスノメソドロジー的分析の知見のシステムデザインでの位置づけられるか、システムデザインのプロセスの中の分析と分析者の位置づけ、またその実例について検討する。

90年代の論文(Suchman, Button, Hughes etc.)
00年代の解説書(Crabtree, Randall)

10年の入門書(Button, Heath)
Button「Studies of work and workplace in HCI」
1.motivation
■Grudinの「HCIのfifth stageはユーザーとの対話だ」はwork settingへの注目を意味するが、それはCSCW、特に社会学と共同した分野である。社会学の中でも、経験的なアプローチが理論より好まれる。
■Suchmanは、従来のHCIにおける認知科学的アプローチ、つまりユーザーを単独で見ることに対抗し、「使用」の社会的文化的状況という視点を導入した。一方、CSCW分野でも、人々の共同作業を促進するには、認知科学的モデルは適切でないことがわかった。Suchmanはそれに対してEMCAによる経験的研究という指針を示した。このほか、スカンジナビアのParticipatory Design運動は、技術開発における、ユーザーの作業状況での使用の重要性を指摘しつづけてきた。
2.Overview: A Paradigmatic Case
■HCIに対するワークの研究の適用は、システムへの批判につながる場合がある。Suchman-Winograd論争の事例。Bowersらの研究では、印刷作業が今までどうだったか、システムが導入されたらどう変わったかを分析した。システムが導入されたら、円滑な共同作業が妨げられてしまった。この原因は、設計者がワークフローを強制してしまったためだった。様々な過程は、状況に合わせられなければならない。そのためにうまくいかせるプロセスがあったはずだが、たまたま起こらなかったためにシステムに反映されなかったのだ。
■ワークの研究は、組織化をうまくいかせるやり方を明らかにする。それは、デザイン方針への批判だけでなく、それをうまくいかせることにもつながる。
4.Detailed description
1.批判:Suchman-Winograd論争
2.評価:Disembodied Conduct→読むか
3.要求定義
4.基礎的関係:Technomethodology

反復型開発と日常的場面、実験
----------------------------


イベントの開催による日常の観察
------------------------------

新技術は、ある日突然日常生活に導入されるわけではなく、いくつかの一般の人間が触れられる領域にまず導入される場合がある。その一つが、エンターテインメントである。エンターテインメント分野は、ユーザーインタフェースやバーチャルリアリティの一般分野での最前線と言える。例えば、最近だと「戦場の絆」や「Kinect」は未来に近い一例である。

このような場はエスノグラファーが新技術が導入された現場を観察できる、貴重な場となりうる。前節までに見てきたように、システム開発においてエスノグラフィーを行う方法は、実験的状況での特定のタスクの観察と、純粋に現在行われている日常の作業場面の観察に分かれている。その2つの折衷策として、Benfordらは、イベントやアート展示などの分析が、実験的な状況と日常生活の架け橋となることを提案している [Benford_2002]_ 。バーチャルリアリティやインタラクティブアートの展示会は、しばしば一般人が新たな技術に触れる機会となる。技術を「展示」することで、Benfordらは以下のような利点があるとしている。

* 外に出すため、技術を曖昧な概念ではなく、詳細な領域まで落とし込める
* 実際の環境で評価できる。公共的な場を研究に巻き込むことが出来、また一般人に新技術のインパクトの理解をプロモートできる。実験では得られない忌憚なき意見も聞ける
* 芸術やエンターテインメントの創造性は、新たなアイデアを育てる土壌となる。また、芸術家の持っている技術を研究に利用できる
([Benford_2002]_ より著者が要約)

一例を挙げる。Crabtreeらは、「Can You See Me Now」という、位置情報ゲームとバーチャルな都市空間を融合させたゲームを開発し、そのイベントを開催することで、多くの一般人が新しい技術に触れる状況を観察した [Crabtree_2004]_ 。彼らは、ゲームの中のRunner(主催側の参加者。GPSを持って街を走り、別のRunnerと協力しながらPlayerから逃げる)をビデオで撮影し、通信を録音し、Player(実際の街を再現した3D環境をFPSのように操作し、Runnerを探す)の行動のログを取った。その結果、(この辺Macからサルベージする必要がある)

HCI研究展示のインタラクティブアート的なあり方
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ところで、実際に日本国内でこのようなイベントが行えるかに関しては、いくつかの問題がある。HCI/CSCW研究を外に出す一つの手段が、学会発表であり、そのうちの幾つかは一般に開かれている。恐らくそのような場でユーザーの観察や、ビデオデータなどの取得は可能であると推測される。例えば、「インタラクティブ東京/IVRC」や「インタラクション」などはその一例であるといえる。

一方で、このような場で多人数を含んだ形でのインタラクションの観察を行うことは難しいと考えられる。というのも、実際にどのようなシステムを扱う論文が通過するか、に関わらず、展示を行うスペースが1ユーザーのインタラクションを想定して設計されているためである。例えば、「インタラクション2011」の「インタラクティブ発表」で通常与えられるスペースは長机2つ程度である。

このような展示の背景には、SIGGRAPHの影響があると考えられる。SIGGRAPHはもともとACMのコンピュータグラフィックスを扱う分科会であったが、2章に見るような技術的変遷から、ヒューマンコンピュータインタラクション、バーチャルリアリティなども扱っている。一方、SIGGRAPHはインタラクティブアートの主要な展示会でもある。実質的に、SIGGRAPH、もしくはそれに類似した学会発表での展示の場は、インタラクティブアートの展示と似通っている。すなわち、数平方mのスペースで1人が鑑賞を行うというスタイルである。

以上のような事情から、現状で拡張現実感を使った遠隔共同作業システムなどを、イベントの形で提示することは難しい。一方、このような状況を打破するような展示の試みは、別の形式の芸術展示から得ることができる。

藤城嘘、黒瀬陽平らによる「カオス*ラウンジ」は、ギャラリーという空間に日常生活そのものを取り入れた展示である。カオスラウンジの全体のコンセプトは、一言でまとめると「Webサービス上で行われるコラボレーションの可視化」である。

例えばpixivなどのWebサービスでは、ユーザーが絵を投稿することができ、それを例えばキャラクターなどのタグによって一覧することができる。一方、このような絵を「芸術表現」と呼ぶのは難しい。タグによって表示される大量の画像の中で、「作者」を鑑賞者が意識することが極めて薄れているためである。一方で、このような状況下で、日々新たな絵が生まれ、コミュニティが曖昧に増殖していく。

一方で、pixivにおける作者は、自らこのような状況で匿名の作品を収集するとともに、それを元に作品を製作していく存在である。

「カオス*ラウンジ」は、このような状況自体を可視化する目的で、つまりWebサービスを媒介にした匿名的なコミュニケーションが、

その要素を色濃く表しているのが、2010年12月に開催された「【新しい】カオス*ラウンジ【自然】」である。この展示では、作品との1対1の対峙としての鑑賞を意図的に排除するように、空間自体が設計されている。


(第3部)

Fieldwork: Geogeo Stamp Rally
===============================
これまで見てきたように、あるシステムが使われる状況をビデオに撮影し、分析するということは必ずしも定型的な作業ではない。本研究では、特定の場面やシステムに対して分析を行うのではなく、複合現実感や位置情報技術など、比較的漠然としたコンセプトでまとめることのできるシステムを、どう分析することができるかということを検討するのが目的である。

(基本的にint2010に出したもののreviceで行く。参考文献やデータなどを再構築する必要)

現在，iPhoneやスマートフォンなどの高度な携帯電話端末が，一般ユーザーに普及している段階にある．これらは，通話やメールなどの枠を遥かに超え，「セカイカメラ」などの位置に対応した情報をカメラ映像に重ねる技術など，従来からMixed Realityと分類されてきた技術を，エンドユーザーにまでもたらしつつある．現在は未だ普及の段階にまで達していないが，実世界とオンラインを結びつける試みに，携帯電話は今後も重要な役割を果たす可能性がある．

一方で，実世界の環境で，携帯端末がどう使われるかに関しては，十分な検討がされていないと見られる．携帯電話には，一人で画面に向き合うだけではなく，例えば電車内で若者が携帯電話に表示されたメール，画像などを見せあっているように，複数人で，場面に応じて共同的に利用するものとしての側面がある．本論文では，実際に携帯端末がどのように複数人によって，実世界の場面の組織化に利用されるかに関して，詳細な分析を行う．

フィールドについて
-------------------

屋外での情報機器の使用を観察する際は，公共のイベントなどの利用が有効である．実際の研究としては，Can You See Me NowというMixed Reality Gameの分析が挙げられる．2009年現在，国内ではその一種と言えるiPhoneを利用した位置情報ゲームが複数行われ始めている．

本研究では，「ジオジオスタンプラリー」という，レーダーのような形式で提示されたポイントの情報やヒントを頼りに，宝探しを行うゲームの調査を行った．これは2009年7月20日に行われた，全体で50人程度が参加したイベントである．

参加者はGPSの専門スタッフ1人を含む5人程度の8つのチームに分かれ，各チームにiPhoneが1台配布された．iPhoneにはDGRadar（図）がインストールされており，それを用いてゲームを行う．DGRadarはGPSで現在位置を取得し，レーダーのように現在位置を中心として，周辺（拡大縮小可）の登録されたポイントへの方角・距離と画像などの付加情報が表示されるアプリケーションである．

実際に行われたゲームは，（１）立教大学キャンパス内での人形探し（２）都電沿線でのスタンプラリーの2つであったが，本論文に関連する前者についてのみ記す．人形は1cm程度の高さのアヒルであり，マグネットによって金属部分に接着可能である．この人形がキャンパス内の5カ所に配置され，それぞれのポイントの位置情報のみがDGRadarに登録された．

各チームはこのアヒルを30分程度で可能な限り見つけるというルールであるが，特に勝敗などを決めるものではなく，純粋に楽しむ目的のものであった．ゲームの終わりに全員集合し，各チームの結果や動いた軌跡などを主催者が発表した．

本イベントには，田島が技術サポートの集団の一人として参加しており，その中で企画者に調査の提案をした．参加者には最初に集合した際に調査内容に関して説明を行い，全員に口頭で撮影の許可を得た．その後，1チームに対して全体で30分程度，小型のデジタルムービーカメラを用いて追跡して撮影を行った．このチームでは，持参のものと含めて2台のiPhoneを用いていた．

分析
----
本研究では，携帯端末の使用を，人々の共同作業の相互行為的な達成の観点で分析した．すなわち，単に一人で画面に向き合い，画面上の情報とインタラクションを図るというだけでなく，周囲の環境/人間と協調しながら，実世界に関係する作業を達成していくという観点である．

共同作業の達成を分析するにあたり，社会学のエスノメソドロジー的な相互行為分析の手法を用いた．これは，ビデオデータなどを用いて，その場に居合わせた人間の会話，指さしなどの身体的な相互行為が，継起的な秩序の中でどのように組織化されるかを分析する手法である．本研究では，特にiPhoneやその使用が，環境の中でどのように見られ，相互行為の中に組み込まれていくかに焦点を当てる．

指さしによる環境の指示
~~~~~~~~~~~~~~~~~~~~~~~
Goodwinは，環境の特定の対象を指す種類の指さしをSymbiotic Gestureとし，会話と全く異なる記号であるが，会話と協調して使われるものとしている．「ジオジオスタンプラリー」で見られた指さしは20件あったが，そのうちの10件がDGRadarを参照した「方角」の指示であった．典型的なものを断片1（図）に示す．以下では，Aの持つiPhoneをiA，Bの持つものをiBとする．

(Datas)

Aは自身のiPhoneを見ながら，次のポイントを発見して報告する．Bはそれを受け，Aの方向を向いて歩き始める．その途中で，AはiPhoneを継続して見ながら，ポイントについてもう一度報告し，一度iPhoneから目を離してポイントの方向を指差し，またiPhoneに視線を戻す．Bはそれを受け，指さしの方向を見てから二人とも歩き始める．

ここで注目する点が，断片1の2,3行目でAが自身のiPhoneを見ているということを，Bが見ているということである（図）．これにより，Bはその後の指さしがDGRadarの提示するポイントを指していることを理解できる．「向こうに」に伴った指さしは，特定の物体や，道路に沿って指したものではない．iPhoneの，方角を提示するDGRadarを見ているということを見た上で，方角を提示していると，意味のある形で理解できるのである．

「方角」と，進むべき「方向」は相互行為の中で明確に区別されていた．DGRadarを見た後の指さしと共に「曲がってってもいいんじゃない」という発話を行い，その後チームで建物を迂回する例が見られた．指さしは表示の方角を指しているが，その先には建物があった．このため，「あっち」「東」などの方角ではなく，「曲がってって」という発話が行われた．方角を，進むべき方向に再構成して発話を行ったのである．

iPhoneを見ているということにより，見ている人の体の向きが，DGRadarの方角を指していると見られた場合があった．ある場面では，Aは最初道路に沿って歩いていたが，iPhoneを覗き込んで横を向いた．それを見た他のメンバーが，向いている方向に歩き始めてしまった．それを受け，Aは「あ，違う，真向こう，真向こう，真向こう，向こう」と訂正を行い，本当にDGRadarが提示している方角を指さす．この場面ではAの見ているiPhoneと，メンバーが利用する資源であるAの体の向きという，2つの異なるエコロジーが問題を起こしている．

以上のように，ジオジオスタンプラリーではiPhoneを見ていることと，指さしや身体的配置は，関連づけられて理解されていた．

2台のiPhoneによる問題解決の試み
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ほとんどのチームで，GPSの精度の問題が発生していた．GPSの誤差は明確には表示されていなかったが，チームの相互行為の中で，複数のiPhoneを用いて明らかにした部分があった．断片2（図）はもともと進んでいた方向の異常に気づき，集合する直前のデータ，断片3（図）は集合してから問題解決を始めたデータである．

(Datas)

当初2人が別のiPhoneを持って歩いており，Aが指さしで先導していた．しかし，BがAの指差しの方向を見て，iBと照らし合わせ，Aに見える形でiBを指差す．Aは止まりiAを見て，BはiBを見ながらAに向かって歩き始める．それを受けてチーム全員が集合する．

集合後，1行目の発話で，Bの胴の向きがAのiPhoneへ向かい始める．Bの「北」の発話の段階では，Bは自身のiBを見ているが，iAを見て「きた？」と言いiAを指差す（図6）．その後ジェスチャーで2台の向きの違いを指摘し，iAの指す方角を聞く．それを受けたAの「イースト」の発話と指さしの後，iPhoneをBに手渡し，並べて見る．そこで初めて，専門家であるCが衛星状態について述べる（13行目）．

注目する点は2つある．まず，どのようにBがAのiPhoneを参照する状況ができたかである．集合前に既にBはiBの異常を示していたが，01行目と胴の動きでiAを見る準備がされている．その後，「北」でiBの表示の具体的な内容を示す．その後の「きた？」でiAを指差したことで，iAとiBの違いが示される．

次が，2台のiPhoneの比較である．iAとiBの表示の違いは理解されていたが，具体的にどう違うのかは，恐らく2台のiPhoneの向きの違いから，直観的にはわかりにくかった．03行目のなぞる動きや，06行目の「どっちなんですか」10行目の「てーと」という疑問がそれを示している．その直後，AはiAをiBと平行になるようにBに渡す（図7）．2つのiPhoneの示す方角は，既に「北」「イースト」で示されている．しかし，精度を問題にする場合，2台を比較可能，つまり平行にすることが必要であった．Cによる専門的な指摘は，2人の比較を見た直後である．

まとめ
------
本調査では，GPSを用いた宝探しゲームの中でiPhoneが環境の中でどのように理解され，複数人の相互行為の中に組織化されていくかを分析した．以下に分析の知見をより一般的な形でまとめる．

* 携帯端末を見たり操作していることは，他の参加者が見ることができ，使用者の身体的相互行為は携帯端末に関連したものとして理解された．
* 身体的配置により，誰かが使っている携帯端末は他の参加者にも利用可能になった．
* 複数の端末などがある場合，それらの配置が問題になり，調整される場合がある．また，それも見ることができる．

本分析の知見は，ゲームという特殊な設定の元でのものであるが，携帯端末を見ながら何かを行うということは，位置情報に限らず表示された文書，画像などに関連したものであることが示唆される．例えば「セカイカメラ」の場合，表示されたエアタグを実際に見なくても，ある程度近くにいれば，体の向きからどの方向のエアタグを見ているのか瞬時に理解できる．

また，例えばiPhoneの場合電子コンパスや加速度センサで，表示を回転させることが可能であるが，これらは持っている人の向きのみを反映でき，他の人間の身体の志向の反映は難しい．場合によっては渡すなどのインタフェース外の相互行為を考慮した設計も必要だろう．このように，本知見を通じて既存のシステムを再検討することも有効である可能性がある．

(オチる)

これによって何がわかったのか？
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
このフィールドで行われたことは、ゲームであり、位置や方向の特定という問題の解決であり、iPhoneの使用である。これらは単純に平行しているわけではなく、例えばゲームで点を取るために位置や方向を特定し、iPhoneを使用することでゲームを進めるなど相互に関係している。本分析でピックアップした断片では、iPhoneの使用を取り巻く指差しなどの身体的相互行為に主に注目した。しかし、これはiPhoneでの情報の提示が間違っているという批判にはならない。また、ゲーム全体に関わるような意思決定も主題としていない。このため、主に位置や方向の特定という問題がどのように解決されるか、ということが本分析の主要な知見だろう。これは、より外部環境のデータをセンシングして、提示するようなシステムでは身振りのあり方を考慮でき、またそれが実際に使用される場面で異なっていくということを示している。この点で、新たなシステムへの要求事項を扱っていると言える。

一方で、この分析では本当にゲームという場面全体を記述できなかったのだろうか。宝探しという主題を元に、我々は様々な場面を想像するだろう。しかし、今回は場面で起こりうる様々な局面を厳密に洗い出し、行為のモデルを作成し、ゲームをデザインしたというわけではない。つまり、ある意味で実際に始まってみないと、ゲームで起こることは予測できないことになる。これはプレイヤーにとっても同様である。この分析で何か場面について分かったものがあったとすれば、それはまだ知られていない事柄である。

そこでまず指摘できるのが、アヒル探しがチームの共同作業として行われたことである。これは注目に値する。例えば完全に障害物がない状況で、GPSの方角指示を元に移動を行ったとしたら、各人は同じ方向に進むため、コミュニケーションは必要ないと思われる。人が集まったら共同作業がされるとは限らない。

そこでゲームを一種の問題解決としてとらえた場合、問題とは何かということを問うことができる。前半のキャンバス内でのアヒル探しと、後半の都電沿線での宝探しではどう問題が異なるだろうか。例えば、ゲームのルールとDGRadarを元にすれば、「方向」の問題は見えてこない。また、GPSの不具合がゲームの障害となることは容易に想像できるが、実際にゲームをどう妨げたのか、また本当に妨げたかどうかには疑問が残る。GPSの問題をお互いに共有して、方向を見定めながら移動するということは、ゲームのルールを破壊するようなことではない。むしろ、ゲーム全体の問題解決の中で、間違えながら試行錯誤していく過程の中にうまく取り込まれている。このように、「iPhoneの位置表示アプリを使った」「宝探しゲーム」の見えない特徴が本分析によって明らかになっている。

この際、本分析はゲームの実際の達成の際の(ゲームのデザインが問題を解決するものではなく、問題をうまく作り出すことにあるという差異はあれど)問題を浮き彫りにしている。これは、ゲームの評価をしているといえ、この結果は例えば方角ではなく方向を提示してみる、GPSにわざと誤差を作っておくなどの、新たなゲームデザインにつなげることができる。

Experiment: Augmented Panorama Viewer
=======================================
本章では、2010年7月に行った実験「パノラマを用いた共同作業」を取り扱う。

コンセプト
----------
遠隔で共同作業を行う手段には、様々なものがある。例えば音声や文字(チャット)、映像などは従来から利用されている。本実験で用いられたものは、その中でも「ものを配置する」ということにフォーカスを当て、そのために「パノラマ」すなわち360度全ての方向を写した映像を利用することを考えた。

この表示の形式は、葛岡、山崎らによる一連のGestureManの研究に影響を受けた。GestureManでは、Body Metaphorという設計思想により、首に配置されたカメラを動かして様々な方向を見ることができる。このため、首の動きを見ることで指示者がどこを見ているか作業者が見ることができ、円滑な指示が可能になる。一方で、現状でロボットは比較的大きなものになるため、作業場所によっては導入できるとは限らない。このため、別のインタラクションを、似たような設計論で実現できないかということを検討した。結果として首を回すかわりにパノラマの提示を、またパノラマを見ている位置を視覚的に提示する方針を採用した。

360度の映像は、以下のような利点から、ものの配置に有用であるように見える。

* 配置を行う場所の全景を見ることができる
* 作業者と物体、配置場所の位置関係を把握することができる
* 作業者に指示を行う際に、場所のどこを指すかをわかりやすく説明できる可能性がある

一方で、以下のような問題も起こる。

* パノラマをどう表示するか？ - パノラマは元々全ての方向を写したものであるため、ただ広げただけでは、位置関係がわかりにくい
* パノラマの特定の部分を見ながら指示をしていることを、どう作業者に伝えるか？

このような問題を解決するために、パノラマを円筒形に表示する形式を採用した。TWISTARに代表される、没入型で360度の視野を確保するシステムでは、人が円筒の中に入り、中から何らかの形で表示された360度の映像を見るという形式をとっている。しかし、この形式では装置が大規模になってしまい、場所をとってしまうという問題がある。このため、本実験で用いた表示形式は、円筒に360度の映像が表示されているのを、外から見る形式を採用した。

これを実現するために、拡張現実感技術を用いた。ここで用いた拡張現実感技術は、ARToolKitというマーカーを使ったシステムで、民生用として一般的に用いられているものである。ARToolKitでは、以下のようなフローで現実空間に3Dの物体を表示する。

* カメラなどで映像のフレームを読み込む
* 画像認識により、マーカーの位置を特定する
* マーカーの位置を原点として、映像に写っている空間の3次元座標を特定する
* 3次元空間に3Dの物体を描画する

この3Dの物体を円筒にし、随時パノラマ映像をテクスチャマッピングすることで、先のような表示形式を実現した。これにより、マーカーが表示された位置に、円筒形のパノラマが表示される。マーカーを見る方向を変えたり、回したりすると、パノラマの別の方向を見ることができる。この方式のもう一つの利点は、パノラマのどこを見ているかを画像処理によって特定できるということである。画面の下方向が3Dのどの方向に当たるかを見ることで、ユーザーがどこを見ているかを推定し、作業者に提示することができる。しかし、この特徴は実際には時間の関係から実装しなかった。

システムの概要
---------------
実際に実装したシステムは、指示者側、作業者側の2つに大きく分かれ、この2つをネットワークで接続することで実現している。

まず、作業者側では、PCにWebカメラが接続され、パノラマ映像のキャプチャと送信を行う。パノラマ映像は、通常は全方位カメラ(Omni-Directional Camera)という特殊なカメラを用いるが、今回は予算の問題から(本研究は一切大学からの予算を用いていない)、市販のWebカメラと半球ミラーから自作した。WebカメラはLogicool QCAM-200Vを用いた。半球ミラーは、新宿東急ハンズで販売されているいくつかの口径のものを試し、直径7cmのものを採用した。まず半球ミラーを机などの上に設置し、Webカメラを真上から見下ろすように、ちょうど良い高さに設定すればパノラマ映像を取得できる。

これを、PCでOpenCVという画像処理ライブラリによってキャプチャし、送信するプログラムを作成した。転送の形式はリアルタイム処理の実現のため、無圧縮でそのままフレームを送信している。

指示者側ではPCに一眼デジタルカメラ(ビデオキャプチャにより接続)が接続され、受け取ったパノラマ映像をARToolKitによってマッピングする処理を行う。一眼デジタルカメラは近くの机に配置され、マーカーを写す。

実験の目的
-----------
上記のようなパノラマを用いた共同作業システムには、いくつかの根本的に不明瞭な点がある。まず、複合現実感を用いたシステムの中でさらに映像合成を行っているため、システムについての理解や、システムを通じた視点の理解がスムーズに行われるのかという問題がある。これはいわゆるユーザビリティに当たる(できれば定量評価でだめな理由)。また、本システムは簡潔で、基礎技術的な位置づけである。これを共同作業に適したシステムにするために、基礎的な技術のみを用いたインタラクションについて理解することが有用である。主にこの2つを目的とする。

実験の概要
-----------
本実験では、ミニチュアの家具を配置するタスクを、指示者、配置者の2名の共同作業によって行った。指示者は家具の配置の写真を見ることができるほか、技術的手段によって設定によっては配置の様子を見ることができる。配置者の前には家具配置スペース(紙によって示されている)と、ばらばらに置かれた家具がある。指示者と配置者は同じ部屋にいるが、お互いを見られないように配置されており、肉声によって会話をしながら家具の配置作業を行う。

指示者の環境設定は、目の前に表示用のPC(MacBook Pro 13inch Early 2009)があり、映像やパノラマ映像が表示される。また、写真表示用のデジタル一眼カメラ(Panasonic DMC-G1)やiPhone 3GS(パノラマ実験ではデジタル一眼カメラがシステムに利用されたためこちらを利用)があり、それぞれ基本的な操作によって写真の閲覧や拡大縮小が可能である。パノラマ実験の場合は、この他にパノラマ操作用にマーカーとマーカー認識用のデジタル一眼カメラが配置されているが、配置は途中で変更した。

配置者の環境設定は、目の前に2つの机があり、手前と奥に配置されている。手前の机では配置するためのA4の用紙や、パノラマ実験の場合は中央にパノラマ用のカメラが配置されている。奥の机には、あらかじめミニチュアの家具がバラバラに置いてある。

実験手順を以下に示す。

* 前の配置を利用しない場合、ミニチュア家具を配置する
* ミニチュア家具の配置の写真を撮影する
* ミニチュア家具をバラバラに奥の机に置く
* 被験者に実験について説明する
* 実験と撮影を開始する
* 指示者と配置者が共同してミニチュア家具を配置する
* 指示者が終わりだと宣言した場合、実験、撮影を終了する

実験は、以下の3つの技術設定で行った。

* 音声のみ:指示者は配置を真上から撮影した写真のみを見ることができ、配置者の状況は会話によってしかわからない。
* 映像:指示者は写真の他に、配置者を斜め上から撮影した映像(カメラ1をそのまま表示したもの)を見ることができる。
* パノラマ映像:指示者は写真の他に、家具配置スペースの中央から撮影したパノラマ映像を、前節で説明したパノラマ映像表示装置によって見ることができる。

以下に、個別の実験の詳細についてまとめた。

======== ============ ====== ====== ======== ============ ============
実験番号 技術設定     指示者 配置者 使用写真 カメラ1      カメラ2
======== ============ ====== ====== ======== ============ ============
1        写真のみ     A      B      1        配置者斜め上 配置者斜め上
2        写真のみ     C      D      2        配置者斜め上 配置者斜め上
3        映像         E      F      3        指示者斜め上 配置者斜め上
4        映像         G      H      4        指示者斜め上 配置者斜め上
5        パノラマ映像 I      J      5        指示者斜め上 配置者斜め上
6        パノラマ映像 J      K      6        指示者斜め上 配置者斜め上
7        パノラマ映像 K      L      7        指示者斜め上 配置者斜め上
8        パノラマ映像 L      M      8        指示者斜め上 配置者斜め上
======== ============ ====== ====== ======== ============ ============

ただし、2,3,4,6,7,8はそれぞれ実験1,2,3,5,6,7の結果を撮影したものである。

実験に使用した写真を以下に示す。

実験1

.. figure:: 6-1-1.eps
   :scale: 50 %

   写真1-1

.. figure:: 6-1-2.eps
   :scale: 50 %

   写真1-2

.. figure:: 6-1-3.eps
   :scale: 50 %

   写真1-3

.. figure:: 6-2-1.eps
   :scale: 50 %

   写真2-1

.. figure:: 6-2-2.eps
   :scale: 50 %

   写真2-2

.. figure:: 6-2-3.eps
   :scale: 50 %

   写真2-3

.. figure:: 6-3-1.eps
   :scale: 50 %

   写真3-1

.. figure:: 6-3-2.eps
   :scale: 50 %

   写真3-2

.. figure:: 6-3-3.eps
   :scale: 50 %

   写真3-3

.. figure:: 6-4-1.eps
   :scale: 50 %

   写真4-1

.. figure:: 6-4-2.eps
   :scale: 50 %

   写真4-2

.. figure:: 6-4-1.eps
   :scale: 50 %

   写真4-1

.. figure:: 6-4-3.eps
   :scale: 50 %

   写真4-3

.. figure:: 6-4-4.eps
   :scale: 50 %

   写真4-4

.. figure:: 6-4-5.eps
   :scale: 50 %

   写真4-5

.. figure:: 6-4-6.eps
   :scale: 50 %

   写真4-6

.. figure:: 6-4-7.eps
   :scale: 50 %

   写真4-7


.. figure:: 6-4-8.eps
   :scale: 50 %

   写真4-8

.. figure:: 6-4-9.eps
   :scale: 50 %

   写真4-9

.. figure:: 6-5-1.eps
   :scale: 50 %

   写真5-1

.. figure:: 6-5-2.eps
   :scale: 50 %

   写真5-2

.. figure:: 6-5-3.eps
   :scale: 50 %

   写真5-3

.. figure:: 6-6-1.eps
   :scale: 50 %

   写真6-1

.. figure:: 6-7-1.eps
   :scale: 50 %

   写真7-1

.. figure:: 6-8-1.eps
   :scale: 50 %

   写真8-1

これによって何がわかったのか？
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
まず前提として挙げておきたいのが、このシステムは元々一つの部屋を領域として、本物の家具と同じ程度の物体を配置することを目的として設計されており、ミニチュアの家具を用いた実験を行ったのは、あくまでそれを擬似的に再現したものであるということである。この場合、「映像を用いた実験」のような設定を行うことは難しくなる。映像を用いた実験では、ミニチュアの家具よりかなり高い場所にカメラが配置され、全体を俯瞰できるようになっている。しかし、実際に部屋にこのようなカメラを配置することは物理的に難しく、例えば監視カメラのような配置だと死角ができるだろう。このため、もし「パノラマを用いた実験」が「映像を用いた実験」より何らかの劣った面があったとしても、それは必ずしもパノラマシステムが劣っていることを意味しない。

また、この実験をミニチュアで行うことが、実際の部屋で家具を配置することと異なる特徴を持つ可能性がありうる。しかし、パノラマ表示インタフェースに関しては、ミニチュア家具、展示会場、都市空間で特に特性が変わらないことを確認している(以下の写真を参照)。あまりに小さすぎる場合だと焦点距離の問題で像がぼやけてしまうが、今回の実験はA4の用紙を配置場所として選択しており、パノラマの周囲4cm(カメラの接近できる限界)には物体が配置されていない。


結果としてのシステムコンセプトと、実装例
========================================

結論
====

.. [Garfinkel1967] Garfinkel, H.,1967, "Studies in Ethnomethodology", Prentice-Hall
.. [Randall2007] Randall, D., et al., 2007, "Fieldwork for Design", Springer
.. [Button2009] Button, G., Sharrock, W., 2009, "Studies of Work and the Workplace in HCI", Morgan amd Claypool
.. [Schegloff2007] Schegloff, E., A., 2007, "Sequence Organization in Interaction: A Primer in Conversation Analysis I", Cambridge University Press
.. [Suchman_2006] Suchman, L., 2006, "Human-Machine Configuration: Plan and Situated Action 2nd Edition", Cambridge University Press
.. [Siio2010] 椎尾一郎, 「ヒューマンコンピュータインタラクション入門」, サイエンス社, 2010
.. [Rekimoto1996] 暦本純一, 「実世界志向インタフェースの研究動向」, コンピュータソフトウェア, Vol.13, No.3, pp.4–18
.. [Ishii2008] Ishii, H., 2008, "Tangible User Interfaces", in "The Human-Computer Interaction Handbook Second Edition", Laurence Eribaum Associates, pp.470-487

.. rubric:: 註
.. [#] 別の手法として、概念分析などがあるがここでは触れない。
